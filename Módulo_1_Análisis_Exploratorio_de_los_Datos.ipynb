{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOVsg58lWv7668bmE3kSJMa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/justorfc/Estadistica_Aplicada/blob/main/M%C3%B3dulo_1_An%C3%A1lisis_Exploratorio_de_los_Datos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Módulo_1 Análisis Exploratorio de los Datos**"
      ],
      "metadata": {
        "id": "yQLr6Dd-9Ech"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CONTENIDOS**\n",
        "\n",
        "I. **Primera Parte: Análisis Univariado**\n",
        "\n",
        ">1. Ecuaciones de las Medidas Descriptivas\n",
        ">2. EDA: Análisis Exploratorio de Datos\n",
        ">3. Probabilidad Frecuentista\n",
        ">4. Variables aleatorias discretas y continua\n",
        ">5. Distribuciones de probabilidad\n",
        ">6. Visualizar la distribución de un conjunto de datos\n",
        ">7. Ajustando Distribuciones de Probabilidad\n",
        ">8. Para Ampliar Conceptos y Código\n",
        ">9. Tarea: Uso de Módulos de Python para EDA\n",
        "\n",
        "II. **Segunda Parte: Análisis Bivariado y Ejercicio con el Cuarteto de Anscombe**\n",
        "\n",
        ">1. METODOLOGIA DEL ANALISIS EXPLORATORIO DE DATOS BIDIMENSIONAL\n",
        ">2.Pruebas de hipótesis\n",
        ">3.Análisis de dos variable y el Cuarteto de Anscombe."
      ],
      "metadata": {
        "id": "51tF_slR9XUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **INTRODUCCIÓN: Módulo_1 Análisis Exploratorio de los Datos**\n",
        "\n",
        "Bienvenidos al módulo \"Análisis Exploratorio de los Datos\" de tu curso de Estadística con Python y R. Exploraremos las fundamentales técnicas y conceptos necesarios para comprender y analizar conjuntos de datos de manera efectiva.\n",
        "\n",
        "El análisis exploratorio de datos (EDA) es la base sobre la cual se construyen todas las etapas del proceso de análisis de datos. Nos permite obtener una visión preliminar y comprensión profunda de nuestros datos antes de emprender cualquier análisis más avanzado. A lo largo de este módulo, desglosaremos los principios clave del EDA y exploraremos cómo aplicarlos utilizando Python y sus bibliotecas especializadas.\n",
        "\n",
        "Desde las bases de la probabilidad frecuentista hasta la visualización de distribuciones de probabilidad y la utilización de herramientas como scipy.stats y el módulo fitter para ajustar distribuciones, este módulo está diseñado para equiparte con las habilidades necesarias para manejar datos de manera rigurosa y efectiva.\n",
        "\n",
        "Ya seas un principiante emocionado por sumergirse en el mundo del análisis de datos o un profesional experimentado que busca refrescar tus conocimientos, este módulo te proporcionará el conocimiento esencial para llevar a cabo un análisis exploratorio sólido y valioso."
      ],
      "metadata": {
        "id": "6yDg0TRSjeiH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Palabras Claves del Módulo Análisis Exploratorio de los Datos**\n",
        "\n",
        "El siguiente es un listado de palabras clave que serán utilizadas en el presente notebook:\n",
        "\n",
        "- Análisis Exploratorio de Datos (EDA)\n",
        "- Probabilidad Frecuentista\n",
        "- Variables Aleatorias Discretas y Continuas\n",
        "- Distribuciones de Probabilidad\n",
        "- Visualización de Datos\n",
        "- Histograma\n",
        "- Gráfico de Densidad\n",
        "- Gráfico de Probabilidad Normal\n",
        "- Ajuste de Distribuciones\n",
        "- `scipy.stats`\n",
        "- Módulo `fitter`\n",
        "- Media, Mediana y Moda\n",
        "- Desviación Estándar\n",
        "- Cuartiles y Percentiles\n",
        "- Diagrama de Dispersión\n",
        "- Correlación\n",
        "- Diagrama de Caja (Box Plot)\n",
        "- Valores Atípicos\n",
        "- Normalización y Estandarización\n",
        "- Matriz de Correlación\n",
        "- Matplotlib\n",
        "- Seaborn\n",
        "- Jupyter Notebook\n"
      ],
      "metadata": {
        "id": "UJi3Kuo8rmBF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Objetivos y Competencias**\n",
        "\n",
        "**Objetivo General:**\n",
        "El objetivo principal de este módulo es proporcionar a los participantes las habilidades esenciales para llevar a cabo un análisis exploratorio de datos sólido y efectivo utilizando herramientas y técnicas de programación en Python. Al finalizar este módulo, los participantes estarán equipados para comprender, visualizar y explorar conjuntos de datos de manera rigurosa, identificando patrones, tendencias y peculiaridades que guiarán análisis más profundos y decisiones informadas.\n",
        "\n",
        "**Competencias Desarrolladas:**\n",
        "\n",
        "1. **Comprensión de EDA:** Los participantes desarrollarán una comprensión sólida de los conceptos fundamentales del Análisis Exploratorio de Datos (EDA), incluyendo la importancia de la comprensión inicial de los datos antes de realizar análisis más avanzados.\n",
        "\n",
        "2. **Manipulación de Datos:** Aprenderán a limpiar y preparar datos para su análisis, incluyendo la detección y manejo de valores atípicos, datos faltantes y errores.\n",
        "\n",
        "3. **Visualización de Datos:** Desarrollarán habilidades en la creación de visualizaciones efectivas para representar distribuciones de datos, patrones de correlación, y otras relaciones relevantes.\n",
        "\n",
        "4. **Uso de Bibliotecas:** Utilizarán bibliotecas como Matplotlib y Seaborn para crear gráficos informativos y presentaciones visuales de datos.\n",
        "\n",
        "5. **Interpretación de Resultados:** Aprenderán a interpretar los resultados de las visualizaciones y a identificar patrones, tendencias y relaciones importantes presentes en los datos.\n",
        "\n",
        "6. **Estadísticas Descriptivas:** Adquirirán conocimientos sobre medidas descriptivas como la media, mediana, moda, desviación estándar, cuartiles y percentiles.\n",
        "\n",
        "7. **Distribuciones de Probabilidad:** Comprenderán diferentes tipos de distribuciones de probabilidad y cómo ajustar estas distribuciones a los datos reales.\n",
        "\n",
        "8. **Utilización de `scipy.stats` y `fitter`:** Aprenderán a utilizar la biblioteca `scipy.stats` y el módulo `fitter` para ajustar distribuciones de probabilidad a los datos y evaluar su idoneidad.\n",
        "\n",
        "9. **Análisis de Correlación:** Desarrollarán habilidades para analizar la correlación entre variables y utilizarán herramientas para identificar relaciones lineales y no lineales.\n",
        "\n",
        "10. **Exploración Avanzada:** Se familiarizarán con técnicas avanzadas como normalización, estandarización y creación de matrices de correlación.\n",
        "\n",
        "Al abordar estos temas y desarrollar estas competencias, los participantes estarán preparados para aplicar el Análisis Exploratorio de Datos en diversas situaciones y serán capaces de tomar decisiones informadas basadas en la comprensión profunda de los datos."
      ],
      "metadata": {
        "id": "g4cr9TGLstC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Plan para Desarrollar el “Módulo_1 Análisis Exploratorio de los Datos”**\n",
        "\n",
        "El siguiente es el plan con breves indicaciones sobre cómo realizar las acciones en el documento de RMarkdown en RStudio Cloud, además de los enlaces relevantes:\n",
        "\n",
        "**Plan para Desarrollar el \"Módulo_1 Análisis Exploratorio de los Datos\" en RStudio Cloud**\n",
        "\n",
        "1. **Crear el Documento de RMarkdown:**\n",
        "   - En RStudio Cloud, crea un nuevo documento de RMarkdown llamado \"Módulo_1 Análisis Exploratorio de los Datos\".\n",
        "   - Utiliza encabezados y formato Markdown para estructurar tu documento y explicar cada sección.\n",
        "\n",
        "2. **Transferir Contenidos del Notebook de Python:**\n",
        "   - Abre el notebook \"ANALISIS DE UNA VARIABLE.ipynb\" en Google Colab desde la URL proporcionada.\n",
        "   - Copia y pega los contenidos relevantes en las celdas de código y texto en tu documento RMarkdown.\n",
        "   - Asegúrate de ajustar la sintaxis y la lógica para que funcione en el entorno de R.\n",
        "\n",
        "3. **Instalar el Paquete \"reticulate\":**\n",
        "   - En una celda de código de tu documento RMarkdown, instala el paquete \"reticulate\" en R mediante la siguiente línea:\n",
        "     ```\n",
        "     install.packages(\"reticulate\")\n",
        "     ```\n",
        "\n",
        "4. **Instalar Paquetes de Python en RStudio:**\n",
        "   - Utiliza una celda de código para instalar los paquetes de Python necesarios. Por ejemplo, para instalar \"seaborn\", escribe:\n",
        "     ```\n",
        "     reticulate::py_install(\"seaborn\")\n",
        "     ```\n",
        "   - Repite este paso para cada paquete mencionado en tu plan.\n",
        "\n",
        "5. **Uso de Módulos de Python para EDA:**\n",
        "   - En tu documento RMarkdown, introduce el uso de módulos de Python para EDA.\n",
        "   - Utiliza la biblioteca \"dataprep\" para realizar exploración de datos. Puedes encontrar información y ejemplos en [este enlace](https://dataprep.ai/).\n",
        "   - Explora otros módulos como \"AutoEDA\", \"pandas-profiling\", \"Sweetviz\" y \"Autoviz\". Encuentra más detalles y ejemplos en [este artículo](https://www.analyticsvidhya.com/blog/2021/08/better-eda-with-3-easy-python-libraries-for-any-beginner/).\n",
        "\n",
        "6. **Crear Visualizaciones y Análisis:**\n",
        "   - Combina tus explicaciones con celdas de código en Python utilizando la notación de \"reticulate\". Esto te permitirá ejecutar código de Python en el mismo documento de RMarkdown.\n",
        "   - Utiliza las bibliotecas de visualización de Python para crear gráficos y análisis exploratorio.\n",
        "\n",
        "Siguiendo este plan, podrás integrar de manera efectiva el análisis exploratorio de datos realizado en Python en tu documento de RMarkdown en RStudio Cloud. Si tienes dudas o enfrentas problemas en algún punto, no dudes en preguntar a ChatGPT."
      ],
      "metadata": {
        "id": "8oUQscpktpAl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gTIgmVvFtZKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sP26pmznxKDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **I. Primera Parte: Análisis Univariado**\n",
        "\n",
        ">1. Ecuaciones de las Medidas Descriptivas\n",
        ">2. EDA: Análisis Exploratorio de Datos\n",
        ">3. Probabilidad Frecuentista\n",
        ">4. Variables aleatorias discretas y continua\n",
        ">5. Distribuciones de probabilidad\n",
        ">6. Visualizar la distribución de un conjunto de datos\n",
        ">7. Ajustando Distribuciones de Probabilidad\n",
        ">8. Para Ampliar Conceptos y Código\n",
        ">9. Tarea: Uso de Módulos de Python para EDA"
      ],
      "metadata": {
        "id": "c3u_ga25wNHx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Ecuaciones de las Medidas Descriptivas**\n",
        "\n",
        "1.1. Definiciones: Población, Muestra, Marco muestral, Parámetro, Estadístico, Concepto de variable, Niveles de medición de las variables.\n",
        "\n",
        "1.2. Medidas de Tendencia Central: Media Generalizada, Mediana, Moda.\n",
        "\n",
        "1.3. Medidas de Dispersión o Variabilidad: Varianza, Desviación Estandar, Coeficiente de Variación.\n",
        "\n",
        "1.4. Variable Centrada y Reducida\n",
        "\n",
        "1.5. Medidas de Posición no Central: Cuantiles y Percentiles\n",
        "\n",
        "1.6. VALORES ATIPICOS (OUTLIERS) Crear Función.\n",
        "\n",
        "1.7. Momentos Potenciales\n",
        "\n",
        "1.8. Medidas de Asimetría y Curtosis\n",
        "\n",
        "1.9. La Regla Empírica\n",
        "\n",
        "1.10. Medidas de Asociación y Correlación"
      ],
      "metadata": {
        "id": "YtUDSMBFzeuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **[1.1. Definiciones: Población, Muestra, Marco muestral, Parámetro, Estadístico, Concepto de variable, Niveles de medición de las variables](https://docs.google.com/document/d/11G_6hBVCBfZt1VAv05rAcktcch-EwQlbGTvIjXRB9CU/edit?usp=sharing).**\n",
        "\n",
        "EL MUESTREO TIENE DOS FASES:## **[1.1. Definiciones: Población, Muestra, Marco muestral, Parámetro, Estadístico, Concepto de variable, Niveles de medición de las variables](https://docs.google.com/document/d/11G_6hBVCBfZt1VAv05rAcktcch-EwQlbGTvIjXRB9CU/edit?usp=sharing).**\n",
        "\n",
        "EL MUESTREO TIENE DOS FASES:\n",
        "1. Cálculo del tamaño de la muestra\n",
        "2. Selección de la Muestra\n",
        "\n",
        "**1. CÁLCULO DEL TAMAÑO DE LA MUESTRA**\n",
        "\n",
        "$$n = \\frac{Z^2*p*q}{e^2+\\frac{Z^2*p*q}{N}}$$\n",
        "\n",
        "Donde:\n",
        "\n",
        "+ $n$.\tTamaño de la muestra.\n",
        "+ $Z$.\tEs el coeficiente de confianza para un nivel de confianza dado. El nivel de confianza se refiere a la probabilidad de que el valor real de un parámetro, se encuentre dentro de los límites especificados en la estimación que se quiere calcular. Los siguientes son los tres niveles de confianza más utilizados y sus valores de Z\n",
        "\n",
        "|Nivel de Confianza|Z|\n",
        "|------------------|-|\n",
        "|90%|1.645|\n",
        "|95%|1.96|\n",
        "|99%|2.58|\n",
        "\n",
        "+ $p$.\tEs la proporción de elementos en la población que tiene determinada característica. Si se desconoce totalmente dicha proporción se tomará $p = 0.5$, lo cual brindará el máximo n para el máximo producto $p*q = 0.5*0.5 = 0.25$ (Recuérdese que $p+q = 1$)\n",
        "+ $e$.\tEl error de muestreo, la tolerancia, el grado de precisión con que se recolectan los datos.\n",
        "+ $N$.\tTamaño de la población.\n",
        "\n",
        "**2. MECANISMOS DE SELECCIÓN DE LA MUESTRA**\n",
        "\n",
        "Una vez que el tamaño de la muestra es calculada se procede a realizar la selección. Para un universo con $N$ elementos y una cantidad n previamente calculada, se escogen $n$ elementos del universo sin reemplazamiento en forma tal que en cada extracción los elementos presentes tengan igual probabilidad de selección.\n",
        "+ MÉTODO COORDINADO NEGATIVO  \n",
        "+ MÉTODO DE FAN-MULLER\n",
        "\n",
        "Ejemplo con un archivo de texto, **separado por punto y coma**: [Base Ventas.txt](https://drive.google.com/file/d/1m-p8ZOVXfmKQ4P_GN95GwcK0ypcpNowL/view?usp=sharing). **Sugerencia**: Utilice pandas y **consulte con ChatGPT, para la creación del código**.\n",
        "\n",
        "1. Convierta el archivo de texto (Base Ventas.txt) en un archivo de Excel (.xlsx) y en un archivo csv (.csv)\n",
        "2. Calcule el tamaño muestral\n",
        "3. Obtenga una muestra con el método coordinado negativo\n",
        "\n",
        "```python\n",
        "# Importar el archivo \"Base Ventas.txt\" con encoding Latin-1\n",
        "df = pd.read_csv(\"Base Ventas.txt\", sep=\";\", encoding=\"Latin-1\")\n",
        "```"
      ],
      "metadata": {
        "id": "Ed57xybwxMIN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kLQ6-PIj1Kay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hzwMSz194eib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **[1.2. Medidas de Tendencia Central: Media Generalizada, Mediana, Moda](https://docs.google.com/document/d/1_LoE0AzKQeeIGi3B97GG6GED0o7jZsYThepxPleF9Uw/edit?usp=sharing)**\n",
        "\n",
        "Las medidas de tendencia central indican el valor central de un conjunto de datos, mientras que las medidas de variabilidad indican qué tan dispersos están los datos alrededor de ese valor central."
      ],
      "metadata": {
        "id": "NGnEgWZBpDzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **[MEDIA GENERALIZADA](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f7/MathematicalMeans.svg/1024px-MathematicalMeans.svg.png)**.\n",
        "\n",
        "Las medias generalizadas, también conocidas como medias de Hölder,  es una abstracción de los diversos tipos de media ([geométrica](http://es.wikipedia.org/wiki/Media_geom%C3%A9trica), [aritmética](http://es.wikipedia.org/wiki/Media_aritm%C3%A9tica), [armónica](http://es.wikipedia.org/wiki/Media_arm%C3%B3nica), etc).\n",
        "\n",
        "[Construcción geométrica para hallar las medias aritmética (A),cuadrática (Q), geométrica (G) y armónica (H) de dos números a y b](https://es.wikipedia.org/wiki/Media_geom%C3%A9trica#/media/Archivo:MathematicalMeans.svg).\n",
        "\n",
        "\n",
        "Se define como:\n",
        "\n",
        "$$x(m)=\\sqrt[m]{\\frac{1}{n}\\sum_{i=1}^n{x_i^m}}, Si\\space m\\neq 0$$\n",
        "\n",
        "$$x(m)=\\sqrt[n]{\\prod_{i=1}^nx_i}, Si\\space m= 0$$\n",
        "\n",
        "En donde el parámetro m indica si la media es:\n",
        "\n",
        "1. **Aritmética**, con m=1\n",
        "\n",
        "$$x(m)=\\sqrt[1]{\\frac{1}{n}\\sum_{i=1}^n{x_i^1}}=\\frac{1}{n}\\sum_{i=1}^nx_i=\\bar{x}$$\n",
        "\n",
        "CODIGO EN R:\n",
        "```r\n",
        "sum(x)/length(x)\n",
        "```\n",
        "\n",
        "R tiene la función `mean()` que calcula el promedio aritmético de un vector numérico.\n",
        "\n",
        "2. **geométrica** con `m=0`\n",
        "\n",
        "$$x(m)=g=\\sqrt[n]{\\prod_{i=1}^nx_i}$$\n",
        "\n",
        "La media geométrica, se define como la raíz N-ésima del producto de los N valores de la distribución.El empleo más frecuente de la media geométrica es el de promediar variables tales como porcentajes, tasas, números índices. etc., es decir, en los casos en los que se supone que la variable presenta variaciones acumulativas.\n",
        "\n",
        "**CÓDIGO EN R**: R tiene la función `prod()` que realiza la productoria y con la cual podemos calcular la media geométrica\n",
        "\n",
        "```r\n",
        "g = prod(x)^(1/n)\n",
        "```\n",
        "\n",
        "3. armónica con `m=-1`\n",
        "\n",
        "$$x(m)=\\sqrt[m]{\\frac{1}{n}\\sum_{i=1}^nx_i^m}=( \\frac{1}{n}\\sum_{i=1}^nx_i^m)^{\\frac{1}{-1}}=\\frac{1}{\\frac{1}{n}\\sum_{i=1}^nx_i^m}$$\n",
        "\n",
        "Obsérvese que la inversa de la media armónica es la media aritmética de los inversos de los valores de la variable. No es aconsejable en distribuciones de variables con valores pequeños. Se suele utilizar para promediar variables tales como productividades, velocidades, tiempos, rendimientos, cambios, etc.\n",
        "\n",
        "**CODIGO EN R**:\n",
        "\n",
        "```r\n",
        "H=(1/n*sum(x^-1))^(1/-1)\n",
        "```\n",
        "\n",
        "4. [cuadrática](http://es.wikipedia.org/wiki/Media_cuadr%C3%A1tica) con `m=2`\n",
        "\n",
        "$$x(m)=Q=\\sqrt[2]{\\frac{1}{n}\\sum_{i=1}^nx_i^2}$$\n",
        "\n",
        "La media cuadrática es la raíz cuadrada de la media aritmética de los cuadrados de los valores de la variable.\n",
        "\n",
        "CODIGO EN R:\n",
        "\n",
        "```r\n",
        "Q=sqrt(1/n*sum(x^2))\n",
        "```\n",
        "\n",
        "Se puede crear una función para generar todas las medias con el siguiente código:\n",
        "\n",
        "```r\n",
        "mtc <- function(x) {\n",
        "\tn=length(x)\n",
        "\tX=sum(x)/n\n",
        "\tG=prod(x)^(1/n)\n",
        "\tH=(1/n*sum(x^-1))^(1/-1)\n",
        "\tQ=sqrt(1/n*sum(x^2))\n",
        "\tmedias=data.frame(X,G,H,Q)\n",
        "\tprint(medias)\n",
        "}\n",
        "```\n",
        "\n",
        "Obsérvese que para valores de m<=0 la expresión sólo tiene sentido si todos los xi>=0. (http://es.wikipedia.org/wiki/Media_generalizada)\n",
        "\n"
      ],
      "metadata": {
        "id": "gBOHxu6uxXV2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MqKjGlMwIXrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Entornos de Ejecución en Google Colab y Uso del Lenguaje R en Cuadernos de Python en Google Colab**\n",
        "\n",
        "**Conociendo el Lenguaje R**. El siguiente es un notebook de Google Colaboratory pero que sólo admite el Lenguaje R. Lo utilizaremos para tener un primer acercamiento a R, para luego combinar R junto con Python en un cuaderno de Python de Google Colab.\n",
        "\n",
        "[Primera Sesión con R y Vectorización](https://colab.research.google.com/drive/1_MUCsoX0O22e9X9wc95gakUsYqlYca9W?usp=sharing). Ya vimos algo de R con este notebook. En estas sesiones veremos R dentro de un notebook de Python\n",
        "\n"
      ],
      "metadata": {
        "id": "cO4jQHtModEG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XZqRWfUHrO4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LQb6jqGhrPg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqbVTV14rcF3"
      },
      "source": [
        "### **Utilizando R dentro de un cuaderno interactivo de Python en Google Colab**\n",
        "\n",
        "En caso de que desee usar Python y R juntos, puede usar R magic para algunas celdas.\n",
        "\n",
        "**NOTA:** Novedad de versiones de RPy2\n",
        "\n",
        "[Conversion 'py2rpy' not defined for objects of type '<class 'str'>'](https://stackoverflow.com/questions/74283327/conversion-py2rpy-not-defined-for-objects-of-type-class-str)\n",
        "\n",
        "**Puede que necesite desinstalar rpy2, use, en una celda de código de Python:**\n",
        "\n",
        "```\n",
        "!pip uninstall rpy2 -y\n",
        "```\n",
        "\n",
        "**Luego instale rpy2 versión 3 ó 3.5.1**\n",
        "\n",
        "```\n",
        "# !pip install rpy2==3.0.0\n",
        "!pip install rpy2==3.5.1\n",
        "```\n",
        "\n",
        "Luego puede activar R magic con:\n",
        "\n",
        "```\n",
        "%load_ext rpy2.ipython\n",
        "```\n",
        "\n",
        "Luego, siempre que desee usar R, comience la celda con %%R\n",
        "\n",
        "```\n",
        "%%R\n",
        "x <- 42\n",
        "print(x)\n",
        "```\n",
        "\n",
        "```\n",
        "%%R\n",
        "x <- 42\n",
        "x\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall rpy2 -y"
      ],
      "metadata": {
        "id": "5eyRBbM2rcF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install rpy2==3.0.0\n",
        "!pip install rpy2==3.5.1"
      ],
      "metadata": {
        "id": "vRJlT9TLrcF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5MGKdXDrcF5"
      },
      "source": [
        "# R Magic\n",
        "%load_ext rpy2.ipython"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wK1GZV3rcF5"
      },
      "source": [
        "%%R\n",
        "x <- 42"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2boiSuyrcF6"
      },
      "source": [
        "%%R\n",
        "x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **[PROPIEDADES DE LA MEDIA ARITMÉTICA](https://docs.google.com/document/d/1_LoE0AzKQeeIGi3B97GG6GED0o7jZsYThepxPleF9Uw/edit?usp=sharing)**\n",
        "\n",
        "1. Propiedades de $\\bar{x}$\n",
        "2. TEOREMA DE KÖNIG\n",
        "3. Cambios de Origen y Escala\n"
      ],
      "metadata": {
        "id": "nrVwHUU4pAcK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CiOLeWGwzfTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **[1.3. Medidas de Dispersión o Variabilidad](https://docs.google.com/document/d/1vmv5zSmD3dnNzCsipSHwl_fMsI0Pxdy7p-tE7GAEVGw/edit?usp=sharing).**\n",
        "\n",
        "Medidas de dispersión. Las medidas de dispersión, también llamadas medidas de variabilidad, muestran la variabilidad de una distribución, indicando por medio de un número, si las diferentes puntuaciones de una variable están muy alejadas de la media. Cuanto mayor sea ese valor, mayor será la variabilidad, cuanto menor sea, más homogénea será a la media. Así se sabe si todos los casos son parecidos o varían mucho entre ellos.\n",
        "\n",
        "1. Varianza\n",
        "2. Desviación Estandar\n",
        "3. Coeficiente de Variación"
      ],
      "metadata": {
        "id": "UYdnYrLqxfgW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i-S3md7wxkrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6WxeSTYJxlB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **[1.4. Variable Centrada y Reducida](https://docs.google.com/document/d/1JrpmYftMffYTqpmYkxQVqtKD97oFyUY_ovdw9PxIsLY/edit?usp=drivesdk)**\n",
        "\n",
        "En análisis de datos, centrar reducir las variables permite comparaciones independientes de la unidad de medida.\n",
        "\n",
        "1. **Centrar una variable** consiste en sustraer su media a cada uno de sus valores inicial.\n",
        "2. **Reducir una variable** consiste en dividir todos sus valores por su desviación típica.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QMF9OaXSxmFB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mneaZqA9xqyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **[1.5. Medidas de Posición no Central: Cuantiles y Percentiles](https://docs.google.com/document/d/1_y2h0v3KDVFDwaut8CvCi5Eb-xG8ykxPrj8j6e8CfVk/edit?usp=sharing)\n",
        "\n",
        "En estadística descriptiva, las medidas de posición no central permiten conocer otros puntos característicos de la distribución que no son los valores centrales. Entre las medidas de posición no central más importantes están los cuartiles.\n",
        "\n"
      ],
      "metadata": {
        "id": "wTFAMfOIxrps"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o4lML7-Nxvgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **[1.6. VALORES ATIPICOS (OUTLIERS) Crear Función](https://docs.google.com/document/d/1peAlnGdDLbebj5v4cfzdIk0q3xCeWdKEmtWMbJpeGTM/edit?usp=sharing).**\n",
        "\n",
        "Los valores atípicos son de dos clases y se pueden definir bajo el criterio que toda observación más alejada de 1.5*IQR del cuarto más cercano, es inusual"
      ],
      "metadata": {
        "id": "N5HBesDuxwPI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8gBXwyffx0dD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **[1.7. Momentos Potenciales](https://docs.google.com/document/d/1FCiDORfg4xvRo8Qi7HFPiezjj9cn4aELBU2fojmPf3A/edit?usp=sharing)**\n",
        "\n",
        "Los momentos de una distribución son valores que la caracterizan de forma que dos distribuciones son iguales si tienen todos sus momentos iguales.  Entre los valores que caracterizan una distribución de frecuencias tenemos los momentos potenciales. Dos distribuciones que tienen sus momentos iguales son iguales, y serán más parecidas cuanto más próximos sean sus momentos. Los momentos suelen considerarse respecto del origen y respecto de la media.\n"
      ],
      "metadata": {
        "id": "IDgfk-4ax1GY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CCIr2fq5x5M5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **[1.8. Medidas de Asimetría y Curtosis](https://docs.google.com/document/d/1N9JT0cKvyHh0mihl2hrDbnSPJlTuV_pq3XTb84FmFpI/edit?usp=sharing)**\n",
        "\n",
        "**Las medidas de asimetría** miden el grado de simetría de una distribución de datos. Una distribución simétrica tiene una forma de campana, con la media, la mediana y la moda en el mismo lugar. Una distribución asimétrica tiene una forma de campana inclinada, con la media, la mediana y la moda en lugares diferentes.\n",
        "\n",
        "**Las medidas de curtosis** miden la forma de la cola de una distribución de datos. Una distribución con una cola larga tiene valores extremos más frecuentes que una distribución con una cola corta.\n",
        "\n",
        "En resumen, las medidas de asimetría nos indican si los datos están distribuidos de manera simétrica o asimétrica, mientras que las medidas de curtosis nos indican si los datos tienen una cola larga o corta.\n",
        "\n",
        "**Ejemplos**\n",
        "\n",
        "* La estatura de las personas es una distribución simétrica. La media, la mediana y la moda están en el mismo lugar, alrededor de 1,70 metros.\n",
        "* El peso de las personas es una distribución asimétrica. La media está por encima de la mediana y la moda, lo que significa que hay más personas pesadas que ligeras.\n",
        "* La riqueza de las personas es una distribución con una cola larga. Hay un número pequeño de personas muy ricas, lo que hace que la distribución tenga una cola larga."
      ],
      "metadata": {
        "id": "WYQ-4ykqx6Ex"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LgaGkw0vx-ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5ubpyh53xlG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **[1.9. La Regla Empírica](https://docs.google.com/document/d/1kd-v7cvV_fAea5qepqabGBjUeHrlqRc1UPBuhvcryn4/edit?usp=sharing)**\n",
        "\n",
        "**La regla empírica**, también conocida como regla 68-95-99,7, es una regla que establece que el 68% de los datos de una distribución normal se encuentran dentro de una desviación estándar de la media, el 95% de los datos se encuentran dentro de dos desviaciones estándar de la media y el 99,7% de los datos se encuentran dentro de tres desviaciones estándar de la media.\n",
        "\n",
        "En otras palabras, la regla empírica nos dice que, en una distribución normal, la mayoría de los datos se encuentran muy cerca de la media, mientras que los valores extremos son más raros.\n",
        "\n",
        "**Ejemplo:**\n",
        "\n",
        "Supongamos que la altura de las personas se distribuye de manera normal con una media de 1,70 metros y una desviación estándar de 0,10 metros. De acuerdo con la regla empírica, el 68% de las personas tendrán una altura entre 1,60 metros y 1,80 metros, el 95% de las personas tendrán una altura entre 1,50 metros y 1,90 metros y el 99,7% de las personas tendrán una altura entre 1,40 metros y 2,00 metros.\n",
        "\n",
        "**Limitaciones:**\n",
        "\n",
        "La regla empírica solo es aplicable a distribuciones normales. Si los datos no se distribuyen de manera normal, la regla empírica puede no ser precisa.\n",
        "\n",
        "**Usos:**\n",
        "\n",
        "La regla empírica se puede utilizar para estimar la probabilidad de que un valor dado se encuentre dentro de un determinado rango. También se puede utilizar para identificar valores extremos."
      ],
      "metadata": {
        "id": "LrRPg8eoyAL5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OCLjswtvyDvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5doU6Bz2xlMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **[1.10. Medidas de Asociación y Correlación_Taller JAEN](https://docs.google.com/document/d/10t_YdyysVr0tKZcSzI_wiqFjatIHpqkby5BMR1CiaBM/edit?usp=sharing)**\n",
        "\n",
        "**Los análisis de asociación entre variables cualitativas** son técnicas estadísticas que se utilizan para determinar si existe una relación entre dos o más variables categóricas. Las técnicas más utilizadas son la **prueba de chi-cuadrado** y la **prueba de odds ratio**.\n",
        "\n",
        "**Los análisis de correlación entre variables numéricas** son técnicas estadísticas que se utilizan para determinar si existe una relación lineal entre dos o más variables numéricas. La técnica más utilizada es el **coeficiente de correlación de Pearson**.\n",
        "\n",
        "En resumen, los análisis de asociación entre variables cualitativas nos indican si existe una relación entre dos o más categorías, mientras que los análisis de correlación entre variables numéricas nos indican si existe una relación lineal entre dos o más valores numéricos.\n",
        "\n",
        "**Ejemplos**\n",
        "\n",
        "* **Análisis de asociación entre variables cualitativas:** ¿Existe una relación entre el sexo y la preferencia política?\n",
        "* **Análisis de correlación entre variables numéricas:** ¿Existe una relación entre la altura y el peso?"
      ],
      "metadata": {
        "id": "8ygmeCF7yFYE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p7WXYOKMyHUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hpna2tKMyJWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrPWq8g5nX30"
      },
      "source": [
        "# **2. EDA: Análisis Exploratorio de Datos Unidimensional**\n",
        "\n",
        "**[EDA: Análisis exploratorio de datos](https://es.wikipedia.org/wiki/An%C3%A1lisis_exploratorio_de_datos)**\n",
        "\n",
        "Las visualizaciones son una herramienta fundamental para entender y compartir ideas sobre los datos. La visualización correcta puede ayudar a expresar una idea central, o abrir un espacio para una más profunda investigación; con ella se puede conseguir que todo el mundo hable sobre un conjunto de datos, o compartir una visión sobre lo que los datos nos quieren decir. [Visualizaciones de datos con Python](https://relopezbriega.github.io/blog/2016/09/18/visualizaciones-de-datos-con-python/)\n",
        "\n",
        "La organización de la información:\n",
        "* [HISTOGRAMAS](https://es.wikipedia.org/wiki/Histograma)\n",
        "* [POLIGONOS DE FRECUENCIAS](https://definicion.de/poligono-de-frecuencia/#:~:text=Pol%C3%ADgono%20de%20frecuencia%20es%20el,mayor%20altura%20de%20estas%20columnas.)\n",
        "* [OJIVAS](https://es.wikipedia.org/wiki/Ojiva_(estad%C3%ADstica)#:~:text=En%20estad%C3%ADstica%2C%20una%20ojiva%20es,y%20la%20frecuencia%20acumulativa%20correspondiente.)\n",
        "\n",
        "Sea X una variable aleatoria de la cual se toma una muestra que consiste de n datos:\n",
        "x1, x2, … xn. Que no han sido ordenados. En la creación de una tabla de distribución de frecuencias, se denominan datos agrupados a los datos dispuestos de manera tabular mediante clases o categorías, dentro de cada una de las cuales caen un número determinado de datos (frecuencia de clase), el siguiente es el procedimiento:\n",
        "\n",
        "1.\tCalcular el número de intervalos, k. Se recomienda que $k<=5*ln(n)$ (en R: $k<=5*log(n)$), hay varios métodos para calcular k, de los siguientes se recomienda tomar la parte entera de:\n",
        "    *\t10*log10(n)\tDixon y Kronmal (1965)\n",
        "    *\t2*n^1/2\t\tVelleman (1976)\n",
        "    *\t1+log2(n)\t\tSturges (1926)\n",
        "    *\t1+3.322*log(n)\tSturges (1926)\n",
        "2.\tCalcular el rango, r = max(x)-min(x)\n",
        "3.\tCalcular el tamaño o extensión de cada intervalo w ≈ r/k\n",
        "    *\tDe tal manera que w*k >=r\n",
        "4.\tHistograma. Marcas de clase, f, h, F, H\n",
        "5.\tPolígono de frecuencia\n",
        "6.\tOjiva\n",
        "\n",
        "\n",
        "|k|linf|lsup|mc|f|h|F|H|\n",
        "|-|----|----|--|-|-|-|-|\n",
        "|1|linf[1]|lsup[1]|mc[1]|f[1]|h[1]|F[1]|H[1]|\n",
        "|2|linf[2]|lsup[2]|mc[2]|f[2]|h[2]|F[2]|H[2]|\n",
        "|…|…|…|…|…|…|…|…|\n",
        "|n|linf[n]|lsup[n]|mc[n]|f[n]|h[n]|F[n]|H[n]|\n",
        "\n",
        "Donde:\n",
        "+ k:\tEl número del intervalo\n",
        "+ linf:\tEl límite Inferior del intervalo k\n",
        "+ lsup:\tEl límite superior del intervalo k\n",
        "+ mc:\tMarca de clase: mc = (linf+lsup)/2\n",
        "+ f;\tFrecuencia Absoluta. Numero de datos que caen entre los límites del intervalo k\n",
        "+ h:\tFrecuencia Relativa. h = f/n\n",
        "+ F:\tFrecuencias Absolutas acumuladas. F[i+1]=F[i]+h[i+1]\n",
        "+ H;\tFrecuencia Relativas acumuladas\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gueZD3kFl2P9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "symngmeqpsjW"
      },
      "source": [
        "# **3. PROBABILIDAD FRECUENTISTA**\n",
        "\n",
        "***La probabilidad es el lenguaje matemático para cuantificar la incertidumbre***\n",
        "\n",
        "FRECUENCIA RELATIVA O  “A POSTERIORI”: Este concepto es conocido como el “Enfoque Frecuentista” o “Método Empírico”.\n",
        "\n",
        "La proximidad de la frecuencia relativa a la probabilidad depende de la repetibilidad de algún proceso y de la posibilidad de contar el número de repeticiones, así como el número de veces que algún evento de interés ocurre.\n",
        "\n",
        "DEFINICION: Si después de N repeticiones de un experimento, es decir, si N es el número total de veces que se realiza el experimento, donde N es muy grande, y NE es el número total de veces que ocurre un evento E, la frecuencia relativa de la ocurrencia de E es NE/N, y la probabilidad P de obtener un evento E es.\n",
        "\n",
        "$$\n",
        "P(E) = \\lim_{N->\\infty}\\frac{N_E}{N}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NOTA:** Novedad de versiones de RPy2\n",
        "\n",
        "[Conversion 'py2rpy' not defined for objects of type '<class 'str'>'](https://stackoverflow.com/questions/74283327/conversion-py2rpy-not-defined-for-objects-of-type-class-str)\n",
        "\n",
        "```\n",
        "!pip uninstall rpy2 -y\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "# !pip install rpy2==3.0.0\n",
        "!pip install rpy2==3.5.1\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "# R Magic\n",
        "%load_ext rpy2.ipython\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "# Esto tiene formato de código\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_sPqO2YT71GI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzqOmkGDaYWf"
      },
      "source": [
        "# En caso de que desee usar Python y R juntos, puede usar R magic para algunas celdas.\n",
        "\n",
        "**NOTA:** Novedad de versiones de RPy2\n",
        "\n",
        "[Conversion 'py2rpy' not defined for objects of type '<class 'str'>'](https://stackoverflow.com/questions/74283327/conversion-py2rpy-not-defined-for-objects-of-type-class-str)\n",
        "\n",
        "**Puede que necesite desinstalar rpy2, use, en una celda de código de Python:**\n",
        "\n",
        "```\n",
        "!pip uninstall rpy2 -y\n",
        "```\n",
        "\n",
        "**Luego instale rpy2 versión 3 ó 3.5.1**\n",
        "\n",
        "```\n",
        "# !pip install rpy2==3.0.0\n",
        "!pip install rpy2==3.5.1\n",
        "```\n",
        "\n",
        "Luego puede activar R magic con:\n",
        "\n",
        "```\n",
        "%load_ext rpy2.ipython\n",
        "```\n",
        "\n",
        "Luego, siempre que desee usar R, comience la celda con %%R\n",
        "\n",
        "```\n",
        "%%R\n",
        "x <- 42\n",
        "print(x)\n",
        "```\n",
        "\n",
        "```\n",
        "%%R\n",
        "x <- 42\n",
        "x\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall rpy2 -y"
      ],
      "metadata": {
        "id": "3qux5_Dg7Gz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install rpy2==3.0.0\n",
        "!pip install rpy2==3.5.1"
      ],
      "metadata": {
        "id": "009h004_6Be4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjljSsZD7DG_"
      },
      "source": [
        "# R Magic\n",
        "%load_ext rpy2.ipython"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrsgP1yy7Cuc"
      },
      "source": [
        "%%R\n",
        "x <- 42"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaqzS873wI93"
      },
      "source": [
        "%%R\n",
        "x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DEMOSTRACIÓN EXPERIMENTAL DE LA PROBABILIDAD FRECUENTISTA\n",
        "\n",
        "**EXPERIMENTO 1**. Se desea simular un experimento consistente en un control automático de refrigeración de seis locales mediante la activación automática de seis unidades de aire acondicionado. Cada aire acondicionado se disparará cuando la temperatura suba a un nivel determinado. Demuestre que en el línite cuando N es muy grande la frecuencia relativa es la probabilidad.\n",
        "\n",
        "**SOLUCIÓN**. Tenemos seis aires acondicionados: A1, A2, A3, A4, A5 y A6. Cada aire acondicionado se disparará cuando el control automático lo determine, luego la frecuencia relativa de que se dispare uno de los seis aires acondicionados es 1/6 = 0.16666666666666666. En R podemos utilizar la función **sample()** para simular el experimento, luego obtendremos las frecuencias absolutas y la frecuencias relativas con las funciones **table()** y **prop.table()**, aumentaremos las veces que se repite el experimento (N, resultados posibles) y miraremos la frecuencia relativa como se acerca al valor de la probabilidad de 1/6 = 0.16666666666666666"
      ],
      "metadata": {
        "id": "rMp1c-D1BwLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos R Magic\n",
        "%load_ext rpy2.ipython"
      ],
      "metadata": {
        "id": "_87k1rdnFA8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AF1cAZ2OwIRa"
      },
      "source": [
        "%%R\n",
        "help(sample)\n",
        "# Función sample()\n",
        "# Random Samples and Permutations\n",
        "# ‘sample’ takes a sample of the specified size from the elements of\n",
        "#     ‘x’ using either with or without replacement."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-DvQIssxaQk"
      },
      "source": [
        "%%R\n",
        "x = 1:12\n",
        "x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSZacACWxaCm"
      },
      "source": [
        "%%R\n",
        "sample(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "sample(x,size=15)\n",
        "# Produce un error"
      ],
      "metadata": {
        "id": "Orlvtd-NFNNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny6jdkJCx0EA"
      },
      "source": [
        "%%R\n",
        "sample(x,size=15, replace=T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yub6IIccxz0v"
      },
      "source": [
        "# Ejemplo de un experimento donde se debe seleccionar un resultado cualquiera de seis resultados posibles\n",
        "%%R\n",
        "x = 1:6\n",
        "x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eaw4Omk4ztrI"
      },
      "source": [
        "%%R\n",
        "1/6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueIJiiqizeyx"
      },
      "source": [
        "%%R\n",
        "table(sample(x, 10, replace=T))\n",
        "# La función table cuenta las frecuencias absolutas de una muestra"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JagUaBGnzenS",
        "collapsed": true
      },
      "source": [
        "%%R\n",
        "prop.table(table(sample(x, 100, replace=T)))\n",
        "# La función prop.table() calcula las frecuencias relativas de una table()\n",
        "\n",
        "# ESTO DEMUESTRA EL CONCEPTO FRECUENTISTA DE LA PROBABILIDAD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGiaCs9UFlvf"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dfuiw1UK20KB"
      },
      "source": [
        "# Hagamoslo ahora con python\n",
        "\n",
        "Repasemos como crear una serie de pandas:\n",
        "\n",
        "[Crear una Serie de Pandas](https://es.acervolima.com/crear-una-serie-de-pandas/)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "list = ['A1', 'A2', 'A3', 'A4', 'A5','A6']\n",
        "\n",
        "ser = pd.Series(list)\n",
        "print(ser)"
      ],
      "metadata": {
        "id": "af73nxzgF5ST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1k6wMyGytRD"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "x = ['A1', 'A2', 'A3', 'A4', 'A5','A6']   # Una lista\n",
        "\n",
        "x = pd.Series(x)    # Debo convertirla en una Serie de Pandas\n",
        "\n",
        "m = x.sample(n=10,replace=True) # Extraigo 10 muestras CON REEMPLAZO\n",
        "                                # AUMENTO N A 100, 1000, 10000, ETC\n",
        "\n",
        "n = len(m)  # Obtengo el número total de resultados posibles\n",
        "\n",
        "print(m.value_counts()/n)   # Cuento las frecuencias relativas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb98FT_JrLaM"
      },
      "source": [
        "# **un histograma tiene como objetivo aproximar la función de densidad (pdf) de probabilidad subyacente que genero los datos agrupando y contando observaciones**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4-M3Z2NCjcA"
      },
      "source": [
        "# **Histograma y Densidad**\n",
        "\n",
        "Una de las mejores maneras de describir una variable es informar los valores que aparecen en el conjunto de datos y cuántas veces aparece cada valor. Esta descripción se llama **la distribución de la variable**. La representación más común de una distribución es un histograma, que es un gráfico que muestra la frecuencia de cada valor. En este contexto, \"frecuencia\" significa el número de veces que aparece el valor.\n",
        "\n",
        "¿Densidad, cómo es eso? La densidad describe como la variable aleatoria tomará determinados valores.\n",
        "\n",
        "Cuando se dibuja un histograma se representarán con las barras las densidades.\n",
        "\n",
        "Con el comando hist(), de R, se puede indicar la densidad agregando el argumento freq:\n",
        "\n",
        "```\n",
        "hist(x, freq=F)\n",
        "```\n",
        "\n",
        "+ **freq = TRUE** indica la frecuencia absoluta (conteo) en el eje y (es el valor por defecto).\n",
        "+ **freq = FALSE** indica la densidad (Frecuencia Relativa o probabilidad). Se dice entonces que se traza un histograma de densidad. Si se quiere trazar la línea de densidad, se debe lanzar la siguiente orden:\n",
        "\n",
        "```\n",
        "lines(density(x))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-UE49Bf5i-M"
      },
      "source": [
        "# Tabla de Distribución de Frecuencias con R\n",
        "\n",
        "```\n",
        "tdf <- function(x) {\n",
        "  # Calcula la Tabla de Distribuci?n de Frecuencias por Intervalos\n",
        "  # Grafica el histograma y la curva de densidad\n",
        "  n=length(x)\n",
        "  k=1+3.322*log10(n)\n",
        "  k=floor(k)\n",
        "  r=max(x,na.rm = T)-min(x,na.rm = T)\n",
        "  w=r/k\n",
        "  w=ceiling(w)\n",
        "  linf=min(x,na.rm = T)\n",
        "  for (i in 1:k)\n",
        "  {\n",
        "    linf[i+1]=linf[i]+w\n",
        "  }\n",
        "  lsup=NA\n",
        "  for (i in 1:k)\n",
        "  {\n",
        "    if (is.integer(x))\n",
        "    {\n",
        "      lsup[i]=linf[i+1]-1\n",
        "    }\n",
        "    else\n",
        "    {\n",
        "      lsup[i]=linf[i+1]-0.01\n",
        "    }\n",
        "  }\n",
        "  f=NA\n",
        "  for (i in 1:k)\n",
        "  {\n",
        "    f[i]=length(which(x>=linf[i] & x<=lsup[i]))\n",
        "  }\n",
        "  linf=linf[1:k]\n",
        "  mc=(lsup+linf)/2\n",
        "  tdf=data.frame(linf=linf,lsup=lsup,mc=mc,f=f)\n",
        "  h=f/sum(f)\n",
        "  tdf=data.frame(linf=linf,lsup=lsup,mc=mc,f=f,h=h)\n",
        "  F=NA\n",
        "  for (i in 1:k)\n",
        "  {\n",
        "    if (i==1)\n",
        "      F[i]=f[i]\n",
        "    else\n",
        "      F[i]=f[i]+F[i-1]\n",
        "  }\n",
        "  tdf=data.frame(tdf,F=F)\n",
        "  H=NA\n",
        "  for (i in 1:k)\n",
        "  {\n",
        "    if (i==1)\n",
        "      H[i]=h[i]\n",
        "    else\n",
        "      H[i]=h[i]+H[i-1]\n",
        "  }\n",
        "  tdf=data.frame(tdf,H=H)\n",
        "  print(tdf)\n",
        "\n",
        "  par(mfrow=c(2,2))\n",
        "  hist(x,freq = F)\n",
        "  density(x,na.rm = T)\n",
        "  hist(x, freq = F)\n",
        "  lines(density(x,na.rm = T))\n",
        "}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3qARdIlRouKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PUEDE QUE NECESITE \"Desconectar y borrar el tiempo de ejecución\" y luego reconectar**"
      ],
      "metadata": {
        "id": "2bfNfb6LoPTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall rpy2 -y"
      ],
      "metadata": {
        "id": "Xo_glElVnRgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install rpy2==3.0.0\n",
        "!pip install rpy2==3.5.1"
      ],
      "metadata": {
        "id": "sqn1iDHPm3Qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos R Magic\n",
        "%load_ext rpy2.ipython"
      ],
      "metadata": {
        "id": "DWZ_FkzEmt2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00US3VWkxZ0U"
      },
      "source": [
        "%%R\n",
        "tdf <- function(x) {\n",
        "  # Calcula la Tabla de Distribuci?n de Frecuencias por Intervalos\n",
        "  # Grafica el histograma y la curva de densidad\n",
        "  n=length(x)\n",
        "  k=1+3.322*log10(n)\n",
        "  k=floor(k)\n",
        "  r=max(x,na.rm = T)-min(x,na.rm = T)\n",
        "  w=r/k\n",
        "  w=ceiling(w)\n",
        "  linf=min(x,na.rm = T)\n",
        "  for (i in 1:k)\n",
        "  {\n",
        "    linf[i+1]=linf[i]+w\n",
        "  }\n",
        "  lsup=NA\n",
        "  for (i in 1:k)\n",
        "  {\n",
        "    if (is.integer(x))\n",
        "    {\n",
        "      lsup[i]=linf[i+1]-1\n",
        "    }\n",
        "    else\n",
        "    {\n",
        "      lsup[i]=linf[i+1]-0.01\n",
        "    }\n",
        "  }\n",
        "  f=NA\n",
        "  for (i in 1:k)\n",
        "  {\n",
        "    f[i]=length(which(x>=linf[i] & x<=lsup[i]))\n",
        "  }\n",
        "  linf=linf[1:k]\n",
        "  mc=(lsup+linf)/2\n",
        "  tdf=data.frame(linf=linf,lsup=lsup,mc=mc,f=f)\n",
        "  h=f/sum(f)\n",
        "  tdf=data.frame(linf=linf,lsup=lsup,mc=mc,f=f,h=h)\n",
        "  F=NA\n",
        "  for (i in 1:k)\n",
        "  {\n",
        "    if (i==1)\n",
        "      F[i]=f[i]\n",
        "    else\n",
        "      F[i]=f[i]+F[i-1]\n",
        "  }\n",
        "  tdf=data.frame(tdf,F=F)\n",
        "  H=NA\n",
        "  for (i in 1:k)\n",
        "  {\n",
        "    if (i==1)\n",
        "      H[i]=h[i]\n",
        "    else\n",
        "      H[i]=h[i]+H[i-1]\n",
        "  }\n",
        "  tdf=data.frame(tdf,H=H)\n",
        "  print(tdf)\n",
        "\n",
        "#  par(mfrow=c(2,2))\n",
        "#  hist(x,freq = F)\n",
        "#  density(x,na.rm = T)\n",
        "#  hist(x, freq = F)\n",
        "#  lines(density(x,na.rm = T))\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dfcfJ3tlGZZ"
      },
      "source": [
        "%%R\n",
        "library(MASS)\n",
        "data(Boston)\n",
        "names(Boston)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "medv = Boston$medv\n",
        "tdf(medv)"
      ],
      "metadata": {
        "id": "l-EvcsoafMaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "summary(medv)"
      ],
      "metadata": {
        "id": "yoOOdBJvgX1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEiSXmLFxoOF"
      },
      "source": [
        "# **4. Variables aleatorias discretas y continua**s.\n",
        "\n",
        "Para comprender de una manera más amplia y rigurosa los tipos de variables, es necesario conocer la definición de conjunto discreto. Un conjunto es discreto si está formado por un número finito de elementos, o si sus elementos se pueden enumerar en secuencia de modo que haya un primer elemento, un segundo elemento, un tercer elemento, y así sucesivamente.\n",
        "\n",
        "**Variable aleatoria discreta**: una v.a. es discreta si su recorrido es un conjunto discreto. La variable Hora del ejemplo anterior es discreta.\n",
        "\n",
        "**Variable aleatoria continua**: una v.a. es continua si su recorrido no es un conjunto numerable. Intuitivamente esto significa que el conjunto de posibles valores de la variable abarca todo un intervalo de números reales. Por ejemplo, la variable que asigna la Humedad Relativa (HR) a un flujo aire caliente entrando en el plenum de un secador de granos, es una variable continua ya que, teóricamente, todo valor entre, pongamos por caso, 10% y 20%, es posible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ut4CFcjKB7L"
      },
      "source": [
        "#### DEFINICIONES. Las siguientes son algunas definiciones de Variable Aleatoria, va:\n",
        "\n",
        "**DEFINICION 1 de Variable Aleatoria**: Una función X de valor real sobre el espacio muestral Ω, se llama “Variable Aleatoria” sobre el Espacio de Probabilidad (Ω, f, P) si para todo conjunto de Borel en Rn, el conjunto:\n",
        "$${X∈B}={w:X(w)∈B}$$\n",
        "Es un elemento de la familia de eventos f.\n",
        "\n",
        "Intuitivamente podemos hablar de una va X como una cantidad que en cada observación aleatoria w, asume un valor real X(w) determinado por esta observación, los eventos de la forma {X€B} donde B es un elemento de Rn, son eventos determinados por condiciones sobre X. (Moreno, Luis, p. 29).\n",
        "\n",
        "**FUNCIÓN DE PROBABILIDAD**. Una función de conjunto no negativa y aditiva P sobre la familia f, recibe el nombre de “medida de probabilidad” o “función de probabilidad”, si P es “normada”; esto es, si:\n",
        "\n",
        "$$PΩ=1$$\n",
        "\n",
        "La tripleta (Ω, f, P), formada por el espacio muestral Ω, la familia de subconjuntos de Ω, y la función de probabilidad P, se conoce con el nombre de “Espacio de Probabilidad”. La escogencia de (Ω, f, P) depende de la información conocida sobre los detalles del experimento y del problema que se va a plantear.\n",
        "\n",
        "f, es una familia de subconjuntos del espacio muestral Ω que cumple las siguientes condiciones:\n",
        "1. φ€f\n",
        "2. $A^c=Ω-A€f$, si A€f\n",
        "3. Si $A_n€f$, para n=1,2,…, entonces $U_{n=1}^\\infty A_n€f$\n",
        "\n",
        "**DEFINICION 2 de Variable Aleatoria**: Si Ω es un espacio muestral con una medida de probabilidad y X es una función con valor real definida en los elementos de Ω, entonces X se denomina Variable Aleatoria. (Wisniewski y Velasco, P. 113)\n",
        "\n",
        "Esto significa que **una va es propiamente una función** que asigna un número real a cada evento de un espacio muestral. Se acostumbra usar letras mayúsculas (Usualmente T, W, X, Y, Z), para denotar variables aleatorias. Por ejemplo, si queremos expresar la probabilidad de que cierta variable aleatoria X asuma algún valor no menor a 4, entonces se escribe así: P (2≤X≤4).\n",
        "\n",
        "**DEFINICION 3**: Una variable aleatoria (v.a.) es un número real asociado al resultado de un experimento aleatorio, es decir, una función real en el espacio muestral, X: Ω→ℜ\n",
        "\n",
        "Una variable aleatoria es una función, que asigna eventos a números reales. Una variable aleatoria o variable estocástica es una variable estadística cuyos valores se obtienen de mediciones en experimentos aleatorios.\n",
        "\n",
        "Los valores posibles de una variable aleatoria pueden representar los posibles resultados de un experimento aún no realizado, o los posibles valores de una cantidad cuyo valor actualmente existente es incierto (p.e., como resultado de medición incompleta o imprecisa). Intuitivamente, una variable aleatoria puede tomarse como una cantidad cuyo valor no es fijo pero puede tomar diferentes valores; una distribución de probabilidad se usa para describir la probabilidad de que se den los diferentes valores. (Wikipedia)\n",
        "\n",
        "Una variable aleatoria puede concebirse como un valor numérico que está afectado por el azar. Dada una variable aleatoria no es posible conocer con certeza el valor que tomará esta al ser medida o determinada, aunque sí se conoce que existe una distribución de probabilidad asociada al conjunto de valores posibles. Por ejemplo, en una epidemia de cólera, se sabe que una persona cualquiera puede enfermar o no (suceso), pero no se sabe cuál de los dos sucesos va a ocurrir. Solamente se puede decir que existe una probabilidad de que la persona enferme. (Wikipedia). Otro ejemplo en un experimento de secado a campo abierto se sabe que el secador puede desempeñarse eficientemente o no (suceso) pero no se sabe cuál de los dos sucesos ocurrirá eficiente o no. Solamente se se puede decir que existe una probabilidad de que el secador se desempeñe eficientemente.\n",
        "\n",
        "Para trabajar de manera sólida con variables aleatorias en general es necesario considerar un gran número de experimentos aleatorios, para su tratamiento estadístico, cuantificar los resultados de modo que se asigne un número real a cada uno de los resultados posibles del experimento. De este modo se establece una relación funcional entre elementos del espacio muestral asociado al experimento y números reales. (Wikipedia).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYpowjlYwie9"
      },
      "source": [
        "Ejemplo: A continuación se muestran los valores de precipitaciones por año en cm.\n",
        "\n",
        "\n",
        "|año|Y(cm)|\n",
        "|---|-----|\n",
        "|1911|101.35|\n",
        "|1912|78.74|\n",
        "|1913|107.44|\n",
        "|1914|106.93|\n",
        "|1915|104.39|\n",
        "|1916|72.90|\n",
        "|1917|42.67|\n",
        "\n",
        "A la función Y que asigna los cm de precipitación a cada año, se le llama variable aleatoria.\n",
        "\n",
        "\n",
        "|Hora|dT|Ec|Ea|Y (Es)|\n",
        "|----|--|--|--|------|\n",
        "|0|3.0|57.0|0.0|0.0|\n",
        "|1|2.0|45.5|-1.3|15.1|\n",
        "|2|4.1|53.7|15.4|30.7|\n",
        "|3|4.0|57.3|13.1|26.5|\n",
        "\n",
        "En la tabla anterior se aprecian valores de Gradientes de temperatura (dT), Eficiencias de Colección en un secador solar (Ec), Eficiencias de absorción de un lecho de piedras (Ea) y eficiencias del sistema secador solar (Es), todas ellas en función de la hora del día (Hora). Estos son ejemplos de variables aleatorias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AZ-zFttx1wX"
      },
      "source": [
        "# **5. DISTRIBUCIONES DE PROBABILIDAD**.\n",
        "\n",
        "Las distribuciones de probabilidad son un concepto fundamental en estadística. Las distribuciones de probabilidad se utilizan tanto a nivel teorico como a nivel práctico.\n",
        "\n",
        "Necesidad: En muchas ocasiones tenemos interés en la probabilidad de que una variable cualquiera tome un valor particular.\n",
        "\n",
        "**DEFINICIÓN**: La distribución de probabilidad de una Variable Aleatoria X es una descripción de las probailidades asociadas con los valores posibles de X. Esta distribución de probabilidad puede expresarse como un listado o tabla, o como una fórmula. (Padilla A. John).\n",
        "\n",
        "EJEMPLO: La siguiente tabla muestra losprimeros datos de un experimento con Maiz de diferentes procedencias.\n",
        "\n",
        "\n",
        "|Raza|Altura|Diametro|Hojas|Endospermo|Grano|Mazorca|\n",
        "|----|------|--------|-----|----------|-----|-------|\n",
        "|CUBANYELLO|120|15|12|6|2|1|\n",
        "|CUBANYELLO|169|11|12|2|1|3|\n",
        "|TUZON|184|17|13|6|2|2|\n",
        "|TUZON|283|19|15|4|3|2|\n",
        "\n",
        "La siguiente podría ser la tabla de distribución de frecuencias del número de hojas por planta:\n",
        "\n",
        "|Hojas|Frecuencia|\n",
        "|-----|----------|\n",
        "|12|8|\n",
        "|13|22|\n",
        "|14|29|\n",
        "|15|13|\n",
        "|16|1|\n",
        "|Total general|73|\n",
        "\n",
        "Se pretende construir la distribución de probabilidad de la variable aleatoria discreta Hojas, donde Hojas es el número de Hojas por planta. La siguiente tabla representa las distribuciones de probablidad.\n",
        "\n",
        "|Hojas|Frecuencia|P(X=x)|\n",
        "|-----|----------|------|\n",
        "|12|8|10.96%|\n",
        "|13|22|30.14%|\n",
        "|14|29|39.73%|\n",
        "|15|13|17.81%|\n",
        "|16|1|1.37%|\n",
        "|Total general|73|100%|\n",
        "\n",
        "En la tabla anterior los valores de X son: x1=12, x2=13. Se calculan las probabilidades para estos valores que no son mas que las frecuencias relativas bajo el concepto frecuentista de la probabilidad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O-YgpWBzxuY"
      },
      "source": [
        "#### Ejemplo: Experimento del mercurio\n",
        "\n",
        "En la tabla siguiente se muestran las frecuencias absolutas y relativas del Hg en el experimento del profesor Luis Dias:\n",
        "\n",
        "|Li|Ls|Hg|f|h|F|H|\n",
        "|--|--|--|-|-|-|-|\n",
        "|0|5|2.5|70|0.8750|70|0.875|\n",
        "|5|10|7.5|4|0.0500|74|0.925|\n",
        "|10|15|12.5|1|0.0125|75|0.9375|\n",
        "|15|20|17.5|4|0.0500|79|0.9875|\n",
        "|20|25|22.5|0|0.0000|79|0.9875|\n",
        "|25|30|27.5|1|0.0125|80|1|\n",
        "\n",
        "Bajo el concepto frecuentista de la probabilidad podemos presentar los intervalos de los valores de Hg con sus respectivas frecuencias relativas de la siguiente forma:\n",
        "\n",
        "|Hg||P(Hg)|\n",
        "|--|-----|-|\n",
        "|Li|Ls||\n",
        "|0|5|0.875|\n",
        "|5|10|0.050|\n",
        "|10|15|0.013|\n",
        "|15|20|0.050|\n",
        "|20|25|0.000|\n",
        "|25|30|0.013|\n",
        "\n",
        "Donde Hg son todos los resultados posibles y P(Hg) es la probabilidad de que en el experimento se obtenga el valor correspondiente de Hg (suceso) en el intervalo dado. Observe que la sumatoria de P(Hg) es igual a 1, lo cual es conforme a los axiomas de la probabilidad.\n",
        "\n",
        "---\n",
        "\n",
        "**El conjunto de pares ordenados (Hg, f(Hg)) es una función de probabilidades**.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Kg0HSPJ11nn"
      },
      "source": [
        "En el ejemplo del número de Hojas por planta en el experimento de Maiz, La variable aleatoria Hojas es una variable aleatoria discreta y a la función de probabilidades P(X=x) se le denomina función de masa de probabilidad.\n",
        "\n",
        "En el ejemplo del mercurio, Hg, es una variable aleatoria continua y la función de probabilidades P(Hg) se le denomina función de densidad de probabilidad y suele representarse por f(x) (Letra minúscula).\n",
        "\n",
        "Cada variable aleatoria, sea discreta o continua, tiene asociada una distribución de probabilidad (aunque esta pudiera ser desconocida), la cual se expresa en general por medio de una fórmula o de alguna tabla.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od-rYZFx149D"
      },
      "source": [
        "#### La Distribución de Probabilidad\n",
        "\n",
        "La Distribución de Probabilidad es una especie de ley matemática que rige el comportamiento estocástico (o aleatorio) de la variable aleatoria en cuestión (Wisniewski y Velasco).\n",
        "\n",
        "En muchos fenómenos naturales (Físicos, químicos o biológicos) y económicos, las distribuciones de probabilidad de las variables aleatorias que intervienen están bien identificadas y estudiadas; en otros fenómenos, semejantes distribuciones son desconocidas y suelen manejarse de manera empírica o aproximada; todavía hay otros fenómenos en los cuales ni siquiera es es posible y suelen enfocarse por medio de otros métodos estaísticos conocidos como No Paramétricos. (Wisniewski y Velasco).\n",
        "\n",
        "Es pertinente señalar que las propiedades y fórmulas para las variables aleatorias discretas y continuas son diferentes, pero se parecen mucho. A menudo, la única diferencia estriba en el cambio de una suma (Σ) por una integral (∫).\n",
        "\n",
        "Con frecuencia nos interesa conocer la probabilidad de que el valor de una variable aleatoria sea menor que o igual a un número real x. De hecho, casi todas las tablas estadísticas así funcionan. Esto se escribe como: F(x) = P(X≤x) y se denomina a esta función definida para todos los números reales x como la Función de Distribución Acumulada (fda), o simplemente distribución acumulada para la variable aleatoria X. (Wisniewski y Velasco). El siguiente cuadro resume las propiedades anteriores:\n",
        "\n",
        "\n",
        "|Caso Discreto|Caso Continuo|\n",
        "|-------------|-------------|\n",
        "|P(X=x)=p(x)=f(x)|f(x) = Expresión matemática de la f.d.p.|\n",
        "|0≤f(x)≤1|0≤f(x)≤1|\n",
        "|$\\sum_{toda x}f(x)=\\sum_{i=1}^n(p_i)=1$|$\\int_{-∞}^{∞}f(x)dx=1$|\n",
        "|F(x)=P(X≤x)=txf(t)|Fx=PX≤x=-∞ftdt|\n",
        "|P(a≤X≤b)=F(b)-F(a)|P(a<X<b)=P(a≤X≤b)=F(b)-F(a)|\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwOb1bb6lQKo"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5HT8ynklQlW"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Histogramas + Densidad](https://picandoconr.wordpress.com/2016/02/21/histogramas-densidad/)\n",
        "\n",
        "Densidad. Cuando algo es más denso es porque existe en una concentración mayor que una referencia."
      ],
      "metadata": {
        "id": "LH9ebInpu1NC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Activamos R Magic\n",
        "%load_ext rpy2.ipython\n",
        "# Luego siempre se desee usar R dentro de Python, comenzar la celda con %%R"
      ],
      "metadata": {
        "id": "TN_eH5KfvaJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "z <- c(3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1,\n",
        "       3.7, 3.4, 3.0, 3.0, 4.0, 4.4, 3.9, 3.5, 3.8, 3.8,\n",
        "       3.4, 3.7, 3.6, 3.3, 3.4, 3.0, 3.4, 3.5, 3.4, 3.2,\n",
        "       3.1, 3.4, 4.1, 4.2, 3.1, 3.2, 3.5, 3.6, 3.0, 3.4,\n",
        "       3.5, 2.3, 3.2, 3.5, 3.8, 3.0, 3.8, 3.2, 3.7, 3.3)"
      ],
      "metadata": {
        "id": "E37ms3UhvWK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Ahora dibujemos un histograma\n",
        "hist(z)"
      ],
      "metadata": {
        "id": "mkGDTPomv9hS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos observar que los valores que más abundan en nuestro conjunto de datos son los que están entre 3 y 4.\n",
        "\n",
        "Bueno, con esto podemos decir que la densidad es mayor entre esos valores.\n",
        "\n",
        "Siguiendo la misma lógica podemos decir que la densidad es menor para los valores entre 2 y 2.5 y entre 4 y 4.5. Simple, ¿cierto?\n",
        "\n",
        "Podemos indicar la densidad en el histograma con un argumento del comando hist. Si revisas la ayuda (?hist) podrás encontrar el argumento freq que puede tomar valores TRUE o FALSE (verdadero o falso), así:\n",
        "\n",
        "+ freq = TRUE indica la frecuencia absoluta (conteo) en el eje y (es el valor por defecto).\n",
        "+ freq = FALSE indica la densidad."
      ],
      "metadata": {
        "id": "Y8PgCuHlwI9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "hist(z, freq = FALSE)"
      ],
      "metadata": {
        "id": "1YPlMqKSwpEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observa que lo único que cambia es el nombre en el eje y (ahora se indica la densidad con la palabra Density).\n",
        "\n",
        "Los histogramas pueden ser confusos cuando vemos el mismo conjunto de datos con diferentes intervalos de clase (categorías). Por ejemplo, podemos cambiar el método para calcular los intervalos de clase «Sturges» (valor por defecto) por el valor «Freedman-Diaconis» (FD)."
      ],
      "metadata": {
        "id": "DyPq2_brwsFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "hist(z, freq = FALSE, breaks = \"FD\")"
      ],
      "metadata": {
        "id": "x91Hg_T4w3jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aunque el histograma cambia se puede seguir observando que la mayoría de valores está entre 3 y 4 ¿verdad?\n",
        "\n",
        "Bueno pues, hay una forma de dibujar la densidad y evitar la confusión con los intervalos de clase.\n",
        "\n",
        "Si queremos conocer la densidad usamos el comando density sobre nuestro conjunto de datos, así:"
      ],
      "metadata": {
        "id": "5zPDCXYbw9xN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "density(z)"
      ],
      "metadata": {
        "id": "xxFTPqzIwrET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si queremos dibujar estos valores simplemente aplicamos el comando plot a todo eso, así:"
      ],
      "metadata": {
        "id": "nKQRmQyPxD_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "plot(density(z))"
      ],
      "metadata": {
        "id": "EITCeInDwCiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esa curva es la curva de densidad de nuestro conjunto de datos.\n",
        "\n",
        "Esta curva es una función matemática y tiene una fórmula que la describe. La curva es el resultado de la unión de puntos [x, y] que se muestran como resumen (valores de x y valores de y) cuando aplicas density(z).\n",
        "\n",
        "Resulta que esta curva no depende de los intervalos por lo que siempre será la misma.\n",
        "\n",
        "Podemos compararla con un histograma de densidad (freq = FALSE como argumento).\n",
        "\n",
        "Primero dibujaremos el histograma (con argumento freq = FALSE) y luego dibujaremos la densidad sobre ese gráfico, sin embargo, presta atención al reemplazo del comando plot por lines cuando grafiquemos la curva de densidad."
      ],
      "metadata": {
        "id": "i0TKdS5qxM-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "hist(z, freq = F)\n",
        "lines(density(z))"
      ],
      "metadata": {
        "id": "kfV-kEqpxmbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿qué pasa si cambio el número de intervalos de clase?\n",
        "\n",
        "Veamos:\n",
        "\n",
        "Ya que vamos a comparar dos métodos («Sturges» y «Freedman-Diaconis») dibujaré los mismos ejes en ambos gráficos (xlim y ylim iguales en cada gráfico)."
      ],
      "metadata": {
        "id": "_3tC-G1ZxwOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "hist(z, freq = F, xlim = c(2, 5), ylim =c(0, 1.1),\n",
        "     breaks = \"Sturges\")\n",
        "\n",
        "lines(density(z))"
      ],
      "metadata": {
        "id": "uG3a2SGDxxsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Método «Freedman-Diaconis»"
      ],
      "metadata": {
        "id": "jYOSz0u-x8pW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "hist(z, freq = F, xlim = c(2, 5), ylim =c(0, 1.1),\n",
        "     breaks = \"FD\")\n",
        "\n",
        "lines(density(z))"
      ],
      "metadata": {
        "id": "uDpdf-Tlx9f-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como puedes ver, el histograma cambia pero no lo hace la curva de densidad :)\n",
        "\n",
        "Incluso podemos crear más intervalos de clase y la curva no cambiará."
      ],
      "metadata": {
        "id": "DLDnra28yJjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "hist(z, freq = F, xlim = c(2, 5), ylim =c(0, 1.1),\n",
        "     breaks = 20)\n",
        "\n",
        "lines(density(z))"
      ],
      "metadata": {
        "id": "YTQ5iWN3yFBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXScI-0QhSJ4"
      },
      "source": [
        "# **6. [VISUALIZAR LA DISTRIBUCION DE UN CONJUNTO DE DATOS CON PYTHON](https://seaborn.pydata.org/tutorial/distributions.html)**\n",
        "\n",
        "Cuando se trata de un conjunto de datos, a menudo lo primero que querrá hacer es tener una idea de cómo se distribuye la variable. Este capítulo del tutorial dará una breve introducción a algunas de las herramientas en seaborn para examinar distribuciones univariadas y bivariadas. También puede consultar el capítulo de [diagramas categóricos](https://seaborn.pydata.org/tutorial/categorical.html#categorical-tutorial) para ver ejemplos de funciones que facilitan la comparación de la distribución de una variable entre niveles de otras variables. (**Un buen ejercicio es crear un cuaderno con el tutorial del link anterior**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HElyUu2itIT"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy import stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZv8Wg7ajHS8"
      },
      "source": [
        "sns.set(color_codes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re7Z4fR-jXUP"
      },
      "source": [
        "#### **Trazar distribuciones univariadas**\n",
        "\n",
        "La forma más conveniente de echar un vistazo rápido a una distribución univariada en seaborn es la función [distplot()](https://seaborn.pydata.org/generated/seaborn.distplot.html#seaborn.distplot).\n",
        "\n",
        "Por defecto, esto dibujará un [histograma](https://es.wikipedia.org/wiki/Histograma) y se ajustará a una estimación de densidad del núcleo ([KDE: kernel density estimation](https://en.wikipedia.org/wiki/Kernel_density_estimation)).\n",
        "\n",
        "La estimación de la densidad del núcleo en el valor $x$ de una variable $X$ se define como:\n",
        "\n",
        "$$\\hat{p}(x)=\\frac{1}{nh}\\sum_{i=1}^{n}K(\\frac{x-x_i}{h})$$\n",
        "\n",
        "donde $\\hat{p}(x)$ es la densidad estimada en el punto $x$, $x_i$ son las $n$ observaciones de la variable y $K$ es una función central, generalmente una función de densidad simétrica de un solo pico, como la densidad normal. La cantidad $h$ se llama ancho de banda y controla el grado de suavidad de la estimación de la densidad: si $h$ es demasiado grande, entonces la estimación de la densidad es suave pero está sesgada como estimador de la densidad verdadera, mientras que si $h$ es demasiado pequeña, entonces el sesgo es bajo. pero la estimación es demasiado aproximada, es decir, la varianza del estimador es grande."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiM91YuQjQ3P"
      },
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "x = np.random.normal(size=1000)\n",
        "\n",
        "sns.distplot(x)\n",
        "# plot a univariate distribution of observations."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5qp5xhjhRNi"
      },
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "x = np.random.binomial(3,0.5,10000)\n",
        "\n",
        "sns.distplot(x);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Czlr4COnMirC"
      },
      "source": [
        "x = np.random.normal(size=100)\n",
        "x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9oeKtHLMica"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNiDp_V3kwo-"
      },
      "source": [
        "#### Histogramas\n",
        "\n",
        "Es probable que los histogramas sean familiares, y ya existe una función [hist()](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.hist.html) en matplotlib. Un histograma representa la distribución de datos formando contenedores a lo largo del rango de los datos y luego dibujando barras para mostrar el número de observaciones que caen en cada contenedor.\n",
        "\n",
        "Para ilustrar esto, eliminemos la curva de densidad y agreguemos un diagrama de alfombra, que dibuja una pequeña marca vertical en cada observación. Puede hacer que la alfombra se trace con la función [rugplot()](https://seaborn.pydata.org/generated/seaborn.rugplot.html#seaborn.rugplot), pero también está disponible en [distplot()](https://seaborn.pydata.org/generated/seaborn.distplot.html#seaborn.distplot):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_RnEZnCztlH"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.random.normal(size=100)\n",
        "\n",
        "plt.hist(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1wHa2__jQuY"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "x = np.random.normal(size=100)\n",
        "\n",
        "sns.distplot(x, kde=False, rug=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Bhk6CVKq9lb"
      },
      "source": [
        "Al dibujar histogramas, la opción principal que tiene es la cantidad de contenedores que debe usar y dónde colocarlos. La función [distplot()](https://seaborn.pydata.org/generated/seaborn.distplot.html#seaborn.distplot) usa una regla simple para adivinar cuál es el número correcto de forma predeterminada, pero probar más o menos contenedores puede revelar otras características en los datos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22BGMRdNjQmk"
      },
      "source": [
        "sns.distplot(x, bins=10, kde=True, rug=True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1GzxHhYbSZq"
      },
      "source": [
        "# Modifiquemos el número de bins\n",
        "sns.distplot(x, bins=6, kde=True, rug=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdqmsVu5bSEr"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbIkrt8BrX6n"
      },
      "source": [
        "#### Estimación de la densidad del núcleo\n",
        "\n",
        "La estimación de la densidad del núcleo puede ser menos familiar, pero puede ser una herramienta útil para trazar la forma de una distribución. Al igual que el histograma, los gráficos de [KDE: kernel density estimation](https://en.wikipedia.org/wiki/Kernel_density_estimation) codifican la densidad de observaciones en un eje con altura a lo largo del otro eje:\n",
        "\n",
        "[Estimación de la densidad de kernel](https://hmong.es/wiki/Kernel_density_estimation)\n",
        "\n",
        "Recordemos que la estimación de la densidad del núcleo en el valor $x$ de una variable $X$ se define como:\n",
        "\n",
        "$$\\hat{p}(x)=\\frac{1}{nh}\\sum_{i=1}^{n}K(\\frac{x-x_i}{h})$$\n",
        "\n",
        "donde $\\hat{p}(x)$ es la densidad estimada en el punto $x$, $x_i$ son las $n$ observaciones de la variable y $K$ es una función central, generalmente una función de densidad simétrica de un solo pico, como la densidad normal. La cantidad $h$ se llama ancho de banda y controla el grado de suavidad de la estimación de la densidad: si $h$ es demasiado grande, entonces la estimación de la densidad es suave pero está sesgada como estimador de la densidad verdadera, mientras que si $h$ es demasiado pequeña, entonces el sesgo es bajo. pero la estimación es demasiado aproximada, es decir, la varianza del estimador es grande."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gmro0NyjQe2"
      },
      "source": [
        "sns.distplot(x, hist=False, rug=True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzxpxP_Rr6sg"
      },
      "source": [
        "Dibujar un KDE está más involucrado computacionalmente que dibujar un histograma. Lo que sucede es que cada observación se reemplaza primero con una [curva normal](https://es.wikipedia.org/wiki/Distribuci%C3%B3n_normal).  centrada en ese valor. Curva también llamada: [gaussiana](https://es.wikipedia.org/wiki/Funci%C3%B3n_gaussiana):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xhmkLs3jQVr"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats   # Contiene muchas distribuciones de probabilidad\n",
        "\n",
        "x = np.random.normal(0, 1, size=30)\n",
        "\n",
        "bandwidth = 1.06 * x.std() * x.size ** (-1 / 5.)\n",
        "\n",
        "support = np.linspace(-4, 4, 200)\n",
        "\n",
        "kernels = []\n",
        "\n",
        "for x_i in x:\n",
        "\n",
        "    kernel = stats.norm(x_i, bandwidth).pdf(support)\n",
        "    kernels.append(kernel)\n",
        "    plt.plot(support, kernel, color=\"r\")\n",
        "\n",
        "sns.rugplot(x, color=\".2\", linewidth=3);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4kl6Q77QCI9"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59HRrxN6vMM7"
      },
      "source": [
        "A continuación, estas curvas se suman para calcular el valor de la densidad en cada punto de la cuadrícula de soporte. La curva resultante se normaliza para que el área debajo de ella sea igual a 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9u_00t1vU_5"
      },
      "source": [
        "from scipy.integrate import trapz\n",
        "\n",
        "density = np.sum(kernels, axis=0)\n",
        "density /= trapz(density, support)\n",
        "plt.plot(support, density);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBTzENPKvfnW"
      },
      "source": [
        "Podemos ver que si usamos la función [kdeplot()](https://seaborn.pydata.org/generated/seaborn.kdeplot.html#seaborn.kdeplot) en seaborn, obtenemos la misma curva. Esta función es utilizada por [distplot()](https://seaborn.pydata.org/generated/seaborn.distplot.html#seaborn.distplot), pero proporciona una interfaz más directa con un acceso más fácil a otras opciones cuando solo desea la estimación de densidad:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgTYwXDbvUnq"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.kdeplot(x);\n",
        "\n",
        "\"\"\"\n",
        "Plot univariate or bivariate distributions using kernel density estimation.\n",
        "\n",
        "A kernel density estimate (KDE) plot is a method for visualizing the\n",
        "distribution of observations in a dataset, analagous to a histogram. KDE\n",
        "represents the data using a continuous probability density curve in one or\n",
        "more dimensions.\n",
        "\n",
        "The approach is explained further in the user guide <tutorial_kde>.\n",
        "\n",
        "Relative to a histogram, KDE can produce a plot that is less cluttered and\n",
        "more interpretable, especially when drawing multiple distributions. But it\n",
        "has the potential to introduce distortions if the underlying distribution is\n",
        "bounded or not smooth. Like a histogram, the quality of the representation\n",
        "also depends on the selection of good smoothing parameters.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwn8melVv5VY"
      },
      "source": [
        "El **parámetro bandwidth (bw)** del KDE controla qué tan ajustada se ajusta la estimación a los datos, de forma muy similar al tamaño del bin en un histograma. Corresponde al ancho de los núcleos que trazamos arriba. El comportamiento predeterminado intenta adivinar un buen valor utilizando una regla de referencia común, pero puede ser útil probar valores más grandes o más pequeños:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UENTnypKwg54"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "sns.kdeplot(x)\n",
        "sns.kdeplot(x, bw=.2, label=\"bw: 0.2\")\n",
        "sns.kdeplot(x, bw=2, label=\"bw: 2\")\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwHnmSXIwzQj"
      },
      "source": [
        "Como puede ver arriba, la naturaleza del proceso Gaussian KDE significa que la estimación se extiende más allá de los valores más grandes y más pequeños en el conjunto de datos. Es posible controlar cuánto más allá de los valores extremos se dibuja la curva con el parámetro **cut**; sin embargo, esto solo influye en cómo se dibuja la curva y no en cómo se ajusta:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMB9YPA0xBSe"
      },
      "source": [
        "sns.kdeplot(x, shade=True, cut=0)\n",
        "sns.rugplot(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTRaOpV1mCI8"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBk5SX9GxLyV"
      },
      "source": [
        "# **7. AJUSTANDO DISTRIBUCIONES DE PROBABILIDAD**\n",
        "\n",
        "El objetivo de la presente sección es ajustar una muestra de un conjunto de datos empíricos a un modelo de distribución teórico. El problema de ajustar distribuciones, puede ser dividido en tres grandes tareas:\n",
        "\n",
        "1. **Seleccionar un modelo teórico**. Este primer paso es bastante informal. En este primer paso pueden ser de mucho interés la estadística descriptiva de los datos, asi como histogramas, indicadores de asimetría y curtosis entre otros.\n",
        "2. **Estimar los parámetros del modelo**. Cada modelo teórico tiene sus propios parámetros, por ejemplo, para la distribución normal, son la media y la desviación estándar. Esta tarea consiste en estimar los parámetros del modelo más probable para el conjunto de datos empíricos.\n",
        "3. **Determinar la significancia del modelo, o la bondad del ajuste**. Este es el paso de mayor complicación, aquí se establece lo bien que los datos observados coinciden con el modelo teórico con los parámetros estimados. Si el nivel de significación computarizada es más allá de un umbral predefinido, la hipótesis de bondad de ajuste se acepta, de lo contrario se rechaza\n",
        "\n",
        "---\n",
        "\n",
        "También puede usar [distplot()](https://seaborn.pydata.org/generated/seaborn.distplot.html#seaborn.distplot) para ajustar una distribución paramétrica a un conjunto de datos y evaluar visualmente qué tan cerca corresponde a los datos observados:\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id_1M7BFjPx3"
      },
      "source": [
        "# Realicemos un ajuste de unos datos ficticios UTILIZANDO EL PARÁMETRO FIT DE displot()\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "x = np.random.gamma(6, size=200)\n",
        "\n",
        "# UTILIZO displot() Y CON EL PARAMETRO fit, le paso la pdf teoríca a que quiero ajustar mis datos\n",
        "sns.distplot(x, kde=False, fit=stats.gamma)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tQeN6tARNZS"
      },
      "source": [
        "dir(stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eLKSXYLQt3Y"
      },
      "source": [
        "# Realicemos un ajuste de unos datos ficticios UTILIZANDO EL PARÁMETRO FIT DE displot()\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "x = np.random.normal(size=100)\n",
        "\n",
        "# UTILIZO displot() Y CON EL PARAMETRO fit, le paso la pdf teoríca a que quiero ajustar mis datos\n",
        "sns.distplot(x, kde=False, fit=stats.norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KSWipQfI_U4"
      },
      "source": [
        "#### MODULO STATS DE SCIPY: [Statistics (scipy.stats)](https://docs.scipy.org/doc/scipy/tutorial/stats.html)\n",
        "\n",
        "**Introducción**\n",
        "\n",
        "Este módulo contiene una gran cantidad de distribuciones de probabilidad, así como una biblioteca cada vez mayor de funciones estadísticas. En esta sección, discutimos las características de **scipy.stats** La intención aquí es proporcionar al usuario un conocimiento práctico de este paquete.\n",
        "\n",
        "Nos referimos al [manual de Referencia](https://docs.scipy.org/doc/scipy/reference/stats.html#statsrefmanual) para obtener más detalles.\n",
        "\n",
        "1. [Distribuciones estadísticas discretas](https://docs.scipy.org/doc/scipy/reference/stats.html#discrete-distributions)\n",
        "2. [Distribuciones estadísticas continuas](https://docs.scipy.org/doc/scipy/reference/stats.html#continuous-distributions)\n",
        "3. [Estadística en Python con SciPy (I)](https://pybonacci.org/2012/04/21/estadistica-en-python-con-scipy/)\n",
        "4. [DISTRIBUCIONES DE PROBABILIDAD CON PYTHON](https://medium.com/@24nars/distribuciones-de-probabilidad-con-python-70b952395c65)\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "**ALGUNAS LECTURAS OMPLEMENTARIAS SOBRE SCIPY**\n",
        "\n",
        "1. [Scipy: computación científica de alto nivel](https://claudiovz.github.io/scipy-lecture-notes-ES/intro/scipy.html)\n",
        "2. [La librería científica Scipy](https://docs.scipy.org/doc/scipy/index.html)\n",
        "3. [Estadística en Python con SciPy (I)](https://pybonacci.org/2012/04/21/estadistica-en-python-con-scipy/)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btMnJb3llj_r"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yg5MRnM48Z-"
      },
      "source": [
        "# **GRAFICAR ALGUNAS DISTRIBUCIONES DE PROBABILIDAD**\n",
        "\n",
        "[Distribuciones estadísticas continuas en: **scipy.stats**](https://docs.scipy.org/doc/scipy/reference/stats.html#continuous-distributions)\n",
        "\n",
        "**Descripción general**. Todas las distribuciones tendrán parámetros de ubicación (L) y Escala (S) junto con los parámetros de forma necesarios, los nombres de los parámetros de forma variarán.\n",
        "\n",
        "+ [Alpha Distribution](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.alpha.html#scipy.stats.alpha). **Implementación: [scipy.stats.alpha](https://github.com/scipy/scipy/blob/v1.9.3/scipy/stats/_continuous_distns.py#L435-L486)**\n",
        "+ [Anglit Distribution](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.anglit.html#scipy.stats.anglit). Implementation: [scipy.stats.anglit](https://github.com/scipy/scipy/blob/v1.9.3/scipy/stats/_continuous_distns.py#L492-L529shttps://github.com/scipy/scipy/blob/v1.9.3/scipy/stats/_continuous_distns.py#L492-L529)\n",
        "+ [Arcsine Distribution](https://docs.scipy.org/doc/scipy/reference/tutorial/stats/continuous_arcsine.html)\n",
        "+ Beta Distribution\n",
        "+ Beta Prime Distribution\n",
        "+ Bradford Distribution\n",
        "+ Burr Distribution\n",
        "+ Burr12 Distribution\n",
        "+ Cauchy Distribution\n",
        "+ Chi Distribution\n",
        "+ Chi-squared Distribution\n",
        "+ Cosine Distribution\n",
        "+ Double Gamma Distribution\n",
        "+ Double Weibull Distribution\n",
        "+ Erlang Distribution\n",
        "+ Exponential Distribution\n",
        "+ Exponentiated Weibull Distribution\n",
        "+ Exponential Power Distribution\n",
        "+ Fatigue Life (Birnbaum-Saunders) Distribution\n",
        "+ Fisk (Log Logistic) Distribution\n",
        "+ Folded Cauchy Distribution\n",
        "+ Folded Normal Distribution\n",
        "+ Fratio (or F) Distribution\n",
        "+ Gamma Distribution\n",
        "+ Generalized Logistic Distribution\n",
        "+ Generalized Pareto Distribution\n",
        "+ Generalized Exponential Distribution\n",
        "+ Generalized Extreme Value Distribution\n",
        "+ Generalized Gamma Distribution\n",
        "+ Generalized Half-Logistic Distribution\n",
        "+ Generalized Inverse Gaussian Distribution\n",
        "+ Generalized Normal Distribution\n",
        "+ Gilbrat Distribution\n",
        "+ Gompertz (Truncated Gumbel) Distribution\n",
        "+ Gumbel (LogWeibull, Fisher-Tippetts, Type I Extreme Value) Distribution\n",
        "+ Gumbel Left-skewed (for minimum order statistic) Distribution\n",
        "+ HalfCauchy Distribution\n",
        "+ HalfNormal Distribution\n",
        "+ Half-Logistic Distribution\n",
        "+ Hyperbolic Secant Distribution\n",
        "+ Gauss Hypergeometric Distribution\n",
        "+ Inverted Gamma Distribution\n",
        "+ Inverse Normal (Inverse Gaussian) Distribution\n",
        "+ Inverted Weibull Distribution\n",
        "+ Johnson SB Distribution\n",
        "+ Johnson SU Distribution\n",
        "+ KSone Distribution\n",
        "+ KStwo Distribution\n",
        "+ KStwobign Distribution\n",
        "+ Laplace (Double Exponential, Bilateral Exponential) Distribution\n",
        "+ Asymmetric Laplace Distribution\n",
        "+ Left-skewed Lévy Distribution\n",
        "+ Lévy Distribution\n",
        "+ Logistic (Sech-squared) Distribution\n",
        "+ Log Double Exponential (Log-Laplace) Distribution\n",
        "+ Log Gamma Distribution\n",
        "+ Log Normal (Cobb-Douglass) Distribution\n",
        "+ Log-Uniform Distribution\n",
        "+ Maxwell Distribution\n",
        "+ Mielke’s Beta-Kappa Distribution\n",
        "+ Nakagami Distribution\n",
        "+ Noncentral chi-squared Distribution\n",
        "+ Noncentral F Distribution\n",
        "+ Noncentral t Distribution\n",
        "+ Normal Distribution\n",
        "+ Normal Inverse Gaussian Distribution\n",
        "+ Pareto Distribution\n",
        "+ Pareto Second Kind (Lomax) Distribution\n",
        "+ Power Log Normal Distribution\n",
        "+ Power Normal Distribution\n",
        "+ Power-function Distribution\n",
        "+ R-distribution Distribution\n",
        "+ Rayleigh Distribution\n",
        "+ Rice Distribution\n",
        "+ Reciprocal Inverse Gaussian Distribution\n",
        "+ Semicircular Distribution\n",
        "+ Student t Distribution\n",
        "+ Trapezoidal Distribution\n",
        "+ Triangular Distribution\n",
        "+ Truncated Exponential Distribution\n",
        "+ Truncated Normal Distribution\n",
        "+ Tukey-Lambda Distribution\n",
        "+ Uniform Distribution\n",
        "+ Von Mises Distribution\n",
        "+ Wald Distribution\n",
        "+ Weibull Maximum Extreme Value Distribution\n",
        "+ Weibull Minimum Extreme Value Distribution\n",
        "+ Wrapped Cauchy Distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMp7rgtO5eWH"
      },
      "source": [
        "#### **EJEMPLO: GRAFICAR LA DISTRIBUCIÓN NORMAL**\n",
        "\n",
        "[Normal Distribution](https://docs.scipy.org/doc/scipy/tutorial/stats/continuous_norm.html)\n",
        "\n",
        "$$\n",
        "f(x) = \\frac{e^{\\frac{-x^2}{2}}}{\\sqrt{2\\pi}}\n",
        "$$\n",
        "\n",
        "**[Implementación: ](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html#scipy.stats.norm)**\n",
        "\n",
        "**scipy.stats.norm**\n",
        "\n",
        "scipy.stats.norm(*args, **kwds) = <scipy.stats._continuous_distns.norm_gen object>\n",
        "\n",
        "Una variable aleatoria continua normal.\n",
        "\n",
        "La palabra clave **loc**  location() especifica la media. La palabra clave **scale** scale() especifica la desviación estándar.\n",
        "\n",
        "**Notas**\n",
        "\n",
        "La función de densidad de probabilidad para **norm** es:\n",
        "\n",
        "$$\n",
        "f(x) = \\frac{e^{\\frac{-x^2}{2}}}{\\sqrt{2\\pi}}\n",
        "$$\n",
        "\n",
        "para un número real $x$.\n",
        "\n",
        "Ejemplos:\n",
        "\n",
        "```\n",
        "from scipy.stats import norm\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots(1, 1)\n",
        "```\n",
        "\n",
        "Calcule algunos primeros momentos:\n",
        "\n",
        "```\n",
        "mean, var, skew, kurt = norm.stats(moments='mvsk')\n",
        "```\n",
        "\n",
        "Muestra la función de densidad de probabilidad ( pdf):\n",
        "\n",
        "```\n",
        "x = np.linspace(norm.ppf(0.01),\n",
        "                norm.ppf(0.99), 100)\n",
        "ax.plot(x, norm.pdf(x),\n",
        "       'r-', lw=5, alpha=0.6, label='norm pdf')\n",
        "```\n",
        "\n",
        "Alternativamente, se puede llamar al objeto de distribución (como una función) para fijar los parámetros de forma, ubicación y escala. Esto devuelve un objeto RV \"congelado\" que mantiene fijos los parámetros dados.\n",
        "\n",
        "Congelar la distribución y mostrar el congelado pdf:\n",
        "\n",
        "```\n",
        "rv = norm()\n",
        "ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
        "```\n",
        "\n",
        "Verifique la precisión de cdfy ppf:\n",
        "\n",
        "```\n",
        "vals = norm.ppf([0.001, 0.5, 0.999])\n",
        "np.allclose([0.001, 0.5, 0.999], norm.cdf(vals))\n",
        "```\n",
        "\n",
        "Genera números aleatorios:\n",
        "\n",
        "```\n",
        "r = norm.rvs(size=1000)\n",
        "```\n",
        "\n",
        "Y compare el histograma:\n",
        "\n",
        "```\n",
        "ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
        "ax.legend(loc='best', frameon=False)\n",
        "plt.show()\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCdsoiSEoIBX"
      },
      "source": [
        "**[ajustar datos a una distribución](https://blog.adrianistan.eu/estadistica-python-ajustar-datos-una-distribucion-parte-vii)**\n",
        "\n",
        "17/01/2018 - Adrián Arroyo Calle\n",
        "\n",
        "Existen modelos que nos hacen la vida más sencilla. ¿Qué pasa si tenemos datos y queremos ver si podemos estar ante un modelo de los ya definidos?\n",
        "\n",
        "Este tipo de ajuste es muy interesante, ya que nos permite saber si los datos en bruto pueden parecerse a los modelos de Normal u otros y aprovecharlo.\n",
        "\n",
        "**Ajuste de datos a una distribución**\n",
        "\n",
        "Para ajustar datos a una distribución, todas las distribuciones continuas de **SciPy** cuentan con la función **fit**. Fit nos devuelve los parámetros con los que ajusta nuestros datos al modelo. ¡Ojo, no nos avisa si es un buen ajuste o no!\n",
        "\n",
        "Ejemplo: Queremos saber si la altura de los hombres adultos del pueblo de Garray sigue una distribución normal. Para ello tomamos una muestra de 80 alturas de hombres adultos en Garray."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bt-In35Dpk-l"
      },
      "source": [
        "altura = [180.55743416791302,\n",
        "159.4830930711535,\n",
        "175.54566032406794,\n",
        "149.06378740901533,\n",
        "140.35494067172635,\n",
        "146.65963134242543,\n",
        "171.34024710764376,\n",
        "140.11601629465872,\n",
        "175.6026441151595,\n",
        "158.00860559393507,\n",
        "122.53612034588375,\n",
        "116.10055909040616,\n",
        "152.89225061770068,\n",
        "148.31372767763455,\n",
        "111.17487190927599,\n",
        "160.18952563680827,\n",
        "151.8729737480722,\n",
        "141.50350042949614,\n",
        "165.2379297612276,\n",
        "150.75979657877465,\n",
        "171.5257501059296,\n",
        "157.97922034080895,\n",
        "159.60144363114716,\n",
        "152.52036681430164,\n",
        "172.0678524550487,\n",
        "163.65457704485283,\n",
        "134.9562174388093,\n",
        "189.70206097599245,\n",
        "153.78203142905076,\n",
        "176.1787894042539,\n",
        "190.83025195589502,\n",
        "199.04182673196726,\n",
        "146.97803776211907,\n",
        "174.22118528139467,\n",
        "170.95045320552694,\n",
        "161.2797407784266,\n",
        "190.61061242859464,\n",
        "168.79257731811308,\n",
        "159.87099716863165,\n",
        "136.22823975268153,\n",
        "166.87622973701335,\n",
        "179.58044852016417,\n",
        "172.49583957582817,\n",
        "165.2662334997042,\n",
        "136.6663345224381,\n",
        "161.9352364324168,\n",
        "174.56164027542448,\n",
        "161.62817356012405,\n",
        "167.65579546297906,\n",
        "170.88930983697742,\n",
        "147.22062198310996,\n",
        "151.85737964663497,\n",
        "158.03323614736198,\n",
        "135.77570282853696,\n",
        "161.25435141827515,\n",
        "193.33084953437478,\n",
        "155.43189514766172,\n",
        "155.89204074847055,\n",
        "179.23931091736836,\n",
        "146.485962651657,\n",
        "166.61617663518228,\n",
        "161.70927578953211,\n",
        "164.89798613982495,\n",
        "139.18195138901498,\n",
        "180.30341647946335,\n",
        "162.4811239647979,\n",
        "171.1035005376699,\n",
        "147.01137545913147,\n",
        "187.03282087175134,\n",
        "172.2476631392949,\n",
        "152.9814634955974,\n",
        "174.43159049461713,\n",
        "174.83877117002814,\n",
        "132.66857703218636,\n",
        "173.98029972846837,\n",
        "133.5435543737402,\n",
        "169.62941676289472,\n",
        "166.4887567852903,\n",
        "138.1150540623029,\n",
        "170.52532661450618]\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "altura = pd.Series(altura)\n",
        "\n",
        "type(altura)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld77Vb9mcRPQ"
      },
      "source": [
        "print(\"Promedio = \", altura.mean());print(\"Desviación Estándar=\",altura.std())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KETK1w6AcQsP"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8Lb2tkLpks2"
      },
      "source": [
        "# Vamos a ajustarlo.\n",
        "import pandas as pd\n",
        "import scipy.stats as ss\n",
        "\n",
        "# df = pd.read_csv(\"alturas.csv\")\n",
        "\n",
        "media, desviacion = ss.norm.fit(altura)\n",
        "\n",
        "print(\"Promedio=\",media) # media = 160,37\n",
        "print(\"Desviación Estándar=\",desviacion) # desviacion = 17,41\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipGuCfkhrote"
      },
      "source": [
        "**En este caso nos informa de que estos datos parecen encajar con los de una distribución normal de media 160,37 y desviación típica 17,41.**\n",
        "\n",
        "¿Cómo de bueno es el ajuste? Kolmogorov\n",
        "\n",
        "Hemos hecho un ajuste, pero no sabemos qué tan bueno es. Existen varios métodos para poner a prueba los ajustes. Existen varios métodos, siendo los más populares Chi-Cuadrado y Kolmogorov-Smirnov.\n",
        "\n",
        "Chi-Cuadrado no se puede aplicar directamente sobre distribuciones continuas, aún así voy a explicar como se haría. Sin embargo, primero vamos a probar con Kolmogorov-Smirnov, que en SciPy es ktest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4FEmvhHr1fL"
      },
      "source": [
        "import pandas as pd\n",
        "import scipy.stats as ss\n",
        "\n",
        "# df = pd.read_csv(\"altura.csv\")\n",
        "\n",
        "media, desviacion = ss.norm.fit(altura)\n",
        "\n",
        "d, pvalor = ss.kstest(altura,\"norm\",args=(media,desviacion)) # \"norm\" hace referencia al nombre en stats\n",
        "# o alternativamente\n",
        "d, pvalor = ss.kstest(altura,lambda x: ss.norm.cdf(x,media,desviacion))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAkjV4for1UF"
      },
      "source": [
        "pvalor\n",
        "# Si pvalor es mayor que 0.05 entonces el ajuste fue bondadoso"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TLCpwFTCytB"
      },
      "source": [
        "# \"norm\" en el kstest() hace referencia al nombre de la cdf en stats\n",
        "from scipy import stats\n",
        "dir(stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faPsc07CDf3t"
      },
      "source": [
        "[Distribuciones estadísticas continuas en: **scipy.stats**](https://docs.scipy.org/doc/scipy/tutorial/stats/continuous.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJ--mtlbEiae"
      },
      "source": [
        "# **LISTAR TODAS LAS DISTRIBUCIONES EN scipy.stats**\n",
        "\n",
        "[Ajuste y selección de distribuciones con Python](https://www.cienciadedatos.net/documentos/pystats01-ajuste-distribuciones-python.html)\n",
        "\n",
        "Además de diferenciar entre distribuciones continuas y discretas, es útil poder seleccionarlas por el rango de valores sobre el que está definida cada distribución (dominio). Por ejemplo, si se quiere modelar la velocidad del viento, aunque no se conozca el tipo exacto de distribución, se puede acotar a aquellas cuyo rango de valores está limitado entre  0  y  +inf .\n",
        "\n",
        "\n",
        "```\n",
        "from scipy import stats\n",
        "import pandas as pd\n",
        "# Distribuciones agrupadas por dominio\n",
        "# ==============================================================================\n",
        "distribuciones = [getattr(stats,d) for d in dir(stats) \\\n",
        "                  if isinstance(getattr(stats,d), (stats.rv_continuous, stats.rv_discrete))]\n",
        "\n",
        "distribucion = []\n",
        "dominio_1 = []\n",
        "dominio_2 = []\n",
        "\n",
        "for dist in distribuciones:\n",
        "    distribucion.append(dist.name)\n",
        "    dominio_1.append(dist.a)\n",
        "    dominio_2.append(dist.b)\n",
        "\n",
        "info_distribuciones = pd.DataFrame({\n",
        "                        'distribucion': distribucion,\n",
        "                        'dominio_1': dominio_1,\n",
        "                        'dominio_2': dominio_2\n",
        "                      })\n",
        "\n",
        "info_distribuciones = info_distribuciones \\\n",
        "                      .sort_values(by=['dominio_1', 'dominio_2'])\\\n",
        "                      .reset_index(drop=True)\n",
        "\n",
        "print(\"-------------------------------------\")\n",
        "print(\"Información distribuciones scipy.stat\")\n",
        "print(\"-------------------------------------\")\n",
        "display(info_distribuciones)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KhIGKIvDduj"
      },
      "source": [
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "# Distribuciones agrupadas por dominio\n",
        "# ==============================================================================\n",
        "distribuciones = [getattr(stats,d) for d in dir(stats) \\\n",
        "                  if isinstance(getattr(stats,d), (stats.rv_continuous, stats.rv_discrete))]\n",
        "\n",
        "distribucion = []\n",
        "dominio_1 = []\n",
        "dominio_2 = []\n",
        "\n",
        "for dist in distribuciones:\n",
        "    distribucion.append(dist.name)\n",
        "    dominio_1.append(dist.a)\n",
        "    dominio_2.append(dist.b)\n",
        "\n",
        "info_distribuciones = pd.DataFrame({\n",
        "                        'distribucion': distribucion,\n",
        "                        'dominio_1': dominio_1,\n",
        "                        'dominio_2': dominio_2\n",
        "                      })\n",
        "\n",
        "info_distribuciones = info_distribuciones \\\n",
        "                      .sort_values(by=['dominio_1', 'dominio_2'])\\\n",
        "                      .reset_index(drop=True)\n",
        "\n",
        "print(\"-------------------------------------\")\n",
        "print(\"Información distribuciones scipy.stat\")\n",
        "print(\"-------------------------------------\")\n",
        "display(info_distribuciones)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for d in distribucion:\n",
        "    print(d)"
      ],
      "metadata": {
        "id": "ooXxJToCbUCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distribucion[1:10]"
      ],
      "metadata": {
        "id": "ju9R3wzQb0pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T80m4W2Tr1KU"
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('min_rows', 100)\n",
        "info_distribuciones"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7FBj7VolrRd"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UN EJERCICIO CON LAS CASAS DE BOSTON\n",
        "\n",
        "# Describa el precio mediano de las casas de boston\n"
      ],
      "metadata": {
        "id": "lSGmb527sytb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wZadPSgr6y5"
      },
      "outputs": [],
      "source": [
        "# 1. Importar los módulos:\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "from scipy import stats"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. # Cargo las casas de boston y el precio promedio de las casas\n",
        "casas = pd.read_csv(\"/content/sample_data/california_housing_train.csv\")\n",
        "casas.median_house_value\n",
        "valores = casas.median_house_value\n",
        "valores.describe()"
      ],
      "metadata": {
        "id": "kQE608KbtFLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. # Muestro el histograma de densidad y su pdf; pdf = función de densidad de probabilidad\n",
        "import seaborn as sns\n",
        "sns.distplot(valores)"
      ],
      "metadata": {
        "id": "BUVIn23StLAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distribucion"
      ],
      "metadata": {
        "id": "drwYzh0fahwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. # Realicemos un ajuste de la variable valores, para gamma. UTILIZANDO EL PARÁMETRO FIT DE displot()\n",
        "\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "# UTILIZO displot() Y CON EL PARAMETRO fit, le paso la pdf teoríca a que quiero ajustar mis datos\n",
        "sns.distplot(valores, kde=False, fit=stats.gamma)"
      ],
      "metadata": {
        "id": "cQ8QDV8ZtRgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Realicemos el ajuste de valores para una normal\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "# UTILIZO displot() Y CON EL PARAMETRO fit, le paso la pdf teoríca a que quiero ajustar mis datos\n",
        "sns.distplot(valores, kde=False, fit=stats.norm)"
      ],
      "metadata": {
        "id": "mKgDVpSstZip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats.f.fit(valores)"
      ],
      "metadata": {
        "id": "_g2Jpi35eaqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. # Hago la prueba de la bondad del ajuste para ver si el ajuste es bueno\n",
        "\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "media, desviacion = stats.norm.fit(valores)\n",
        "\n",
        "d, pvalor = stats.kstest(valores,\"norm\",args=(media,desviacion)) # \"norm\" hace referencia al nombre en stats\n",
        "# Perform the Kolmogorov-Smirnov test for goodness of fit.\n",
        "\n",
        "# This performs a test of the distribution F(x) of an observed\n",
        "# random variable against a given distribution G(x).\n",
        "\n",
        "pvalor"
      ],
      "metadata": {
        "id": "In5_7FLethkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "paramaetros = stats.gamma.fit(valores)\n",
        "\n",
        "stats.kstest(valores,\"gamma\",args=paramaetros)"
      ],
      "metadata": {
        "id": "8e2Ej5dMe0_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wml0IIox8wnd"
      },
      "source": [
        "# **EJERCICIOS SOBRE: \"Visualización y Ajuste de la distribución de un conjunto de datos\"**\n",
        "\n",
        "1.   Cargue el conjunto de datos acero. Visualice las variables: consumo y ProdTotal.\n",
        "2.   Cargue el conjunto de datos  agregados y visualice las variables: rcompa, rcompc y denscon.\n",
        "3.   Cargue el conjunto de datos embalajes y visualice las variables: ph, acidez, humedad\n",
        "\n",
        "---\n",
        "\n",
        "4. Para todos los puntos anteriores ajuste distribuciones de probabilidad para cada conjunto de datos.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h_BsCfcB3qw"
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnLKwrdqB3Sq"
      },
      "source": [
        "# Cargo las casas de boston\n",
        "casas = pd.read_csv(\"/content/sample_data/california_housing_train.csv\")\n",
        "casas.median_house_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXEtlJkWZEDK"
      },
      "source": [
        "valores = casas.median_house_value\n",
        "valores.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t875y365Z1nA"
      },
      "source": [
        "# Muestro el histograma de densidad y su pdf\n",
        "# pdf = función de densidad de probabilidad\n",
        "import seaborn as sns\n",
        "sns.distplot(valores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuFdbQwzaIH1"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwU7O_eoZgNC"
      },
      "source": [
        "# Realicemos el ajuste de valores\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "# UTILIZO displot() Y CON EL PARAMETRO fit, le paso la pdf teoríca a que quiero ajustar mis datos\n",
        "sns.distplot(valores, kde=False, fit=stats.gamma)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHidlc63ZD3p"
      },
      "source": [
        "# Realicemos el ajuste de valores para una normal\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "# UTILIZO displot() Y CON EL PARAMETRO fit, le paso la pdf teoríca a que quiero ajustar mis datos\n",
        "sns.distplot(valores, kde=False, fit=stats.norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAMhe3ZUbhw-"
      },
      "source": [
        "# Hago la prueba para ver si el ajuste es bueno\n",
        "\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "paramatros = stats.norm.fit(valores)\n",
        "\n",
        "d, pvalor = stats.kstest(valores,\"norm\",args=paramatros) # \"norm\" hace referencia al nombre en stats\n",
        "# o alternativamente\n",
        "# d, pvalor = ss.kstest(altura,lambda x: ss.norm.cdf(x,media,desviacion))\n",
        "pvalor\n",
        "\n",
        "# La hipótesis nula es: valores es normal\n",
        "# Desición: si pvalor es menor que 0.05 entonces la afirmación anterior se rechaza\n",
        "if pvalor < 0.05:\n",
        "    print(pvalor, \" Es menor que 0.05 entonces no es normal\")\n",
        "elif pvalor > 0.05:\n",
        "    print(pvalor, \" Es mayor que 0.05 entonces SI es normal\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtbMW58JB2qB"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tfqEqFboEBB"
      },
      "source": [
        "# **8. PARA AMPLIAR CONCEPTOS Y CODIGO**\n",
        "\n",
        "1.   [Ajuste y selección de distribuciones con Python](https://www.cienciadedatos.net/documentos/pystats01-ajuste-distribuciones-python.html)\n",
        "2.   [FITTER documentation](https://fitter.readthedocs.io/en/latest/). EJERCICIO: CREAR UN CUADERNO CON LA DOCUMENTACION DE fitter.\n",
        "3.   [Distribuciones de probabilidad con Python](https://relopezbriega.github.io/blog/2016/06/29/distribuciones-de-probabilidad-con-python/)\n",
        "4.   [Estadística en Python: ajustar datos a una distribución, parte VII](https://blog.adrianistan.eu/estadistica-python-ajustar-datos-una-distribucion-parte-vii)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8z_j3qAlqRsy"
      },
      "source": [
        "#### **INTENTEMOS ALGO CON fitter**\n",
        "\n",
        "\n",
        "El paquete fitter proporciona una clase simple para identificar la distribución a partir de la cual se generan las muestras de datos. Utiliza 80 distribuciones de Scipy y le permite trazar los resultados para verificar cuál es la distribución más probable y los mejores parámetros.\n",
        "\n",
        "**Instalación:**\n",
        "\n",
        "```\n",
        "pip install fitter\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ6g_RMnpuDn"
      },
      "source": [
        "!pip install fitter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0mMp6Wko6Wc"
      },
      "source": [
        "from scipy import stats\n",
        "\n",
        "# Primero, creemos muestras de datos con N = 10,000 puntos a partir de una distribución gamma:\n",
        "data = stats.gamma.rvs(2, loc=1.5, scale=2, size=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlzxbdyRo7R-"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu2mKxuzpBjG"
      },
      "source": [
        "Ahora, sin ningún conocimiento sobre la distribución o su parámetro, ¿cuál es la distribución que mejor se ajusta a los datos? Scipy tiene 80 distribuciones y la clase Fitter las escaneará todas, llamará a la función de ajuste por usted, ignorando aquellas que fallan o se ejecutan para siempre y finalmente le dará un resumen de las mejores distribuciones en el sentido de la suma de los errores cuadrados. Lo mejor es dar un ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttLWYXgmpG2P"
      },
      "source": [
        "# fitter debe instalarce\n",
        "!pip install fitter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VPdd6l_o8k1"
      },
      "source": [
        "# Como se usa fitter\n",
        "\n",
        "from fitter import Fitter\n",
        "\n",
        "f = Fitter(data)\n",
        "\n",
        "f.fit()\n",
        "# may take some time since by default, all distributions are tried\n",
        "# but you call manually provide a smaller set of distributions\n",
        "f.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distribucion"
      ],
      "metadata": {
        "id": "MYazrEplh9RO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(valores,kde=False, fit=stats.gamma)"
      ],
      "metadata": {
        "id": "6W8eYmQEi43V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(valores,kde=False, fit=stats.f)"
      ],
      "metadata": {
        "id": "qOVMPm90ifhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp-4FsFO0S9i"
      },
      "source": [
        "from scipy import stats\n",
        "# data = stats.gamma.rvs(2, loc=1.5, scale=2, size=100000)\n",
        "\n",
        "from fitter import Fitter\n",
        "\n",
        "f = Fitter(data, distributions=['f','gamma', 'logistic','rayleigh', 'johnsonsb'])\n",
        "f.fit()\n",
        "f.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exporto valores a un archivo de excel\n",
        "import pandas as pd\n",
        "\n",
        "valores.to_csv(\"valores.csv\")"
      ],
      "metadata": {
        "id": "wSmgsB5EjCs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kqY4JWLNxOuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3UFH73AOxOe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9. Tarea. Uso de Módulos de Python para EDA:**\n",
        "\n",
        "   - En tu documento RMarkdown, introduce el uso de módulos de Python para EDA.\n",
        "   - Utiliza la biblioteca \"dataprep\" para realizar exploración de datos. Puedes encontrar información y ejemplos en [este enlace](https://dataprep.ai/).\n",
        "   - Explora otros módulos como \"AutoEDA\", \"pandas-profiling\", \"Sweetviz\" y \"Autoviz\". Encuentra más detalles y ejemplos en [este artículo](https://www.analyticsvidhya.com/blog/2021/08/better-eda-with-3-easy-python-libraries-for-any-beginner/)."
      ],
      "metadata": {
        "id": "ic6Syl5CxPnh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IL5TD2lm8WXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Xcejb_nA9qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **II. SEGUNDA PARTE: Análisis Bivariado y Ejercicio con el Cuarteto de Anscombe**"
      ],
      "metadata": {
        "id": "gURRT7lXHmbd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. METODOLOGIA DEL ANALISIS EXPLORATORIO DE DATOS BIDIMENSIONAL**\n",
        "\n",
        "El análisis bivariado se utiliza para examinar la relación entre dos variables, ya sean numéricas o categóricas. A continuación se presenta una metodología clara y entendible para realizar este tipo de análisis:\n",
        "\n",
        "### 1) Variables Cuantitativas\n",
        "\n",
        "En el caso de tener dos variables cuantitativas, se estudiará la asociación lineal mediante el método de correlación. Para medir la fuerza de esta relación lineal, se utiliza el coeficiente de correlación de Pearson (r). A diferencia de la covarianza, este coeficiente es independiente de la escala de medida de las variables.\n",
        "\n",
        "El coeficiente de correlación de Pearson puede variar en un rango de -1 a 1, donde el signo indica la dirección de la asociación. Los valores -1 y 1 representan asociaciones más fuertes, mientras que un valor de 0 indica falta de asociación. Es importante calcular la significancia estadística del coeficiente de correlación para determinar si la asociación es estadísticamente significativa.\n",
        "\n",
        "En el análisis, se pueden utilizar las funciones `cor()` y `cor.test()` del paquete estadístico R para calcular el coeficiente de correlación de Pearson y su significancia. Estas funciones también permiten probar la normalidad de los datos mediante la prueba de Shapiro-Wilk.\n",
        "\n",
        "En caso de que los datos no sigan una distribución normal, se puede utilizar el coeficiente de correlación τ (tau) de Kendall o el coeficiente de spearman, que son métodos no paramétrico para calcular la asociación.\n",
        "\n",
        "### 2) Variables Categóricas\n",
        "\n",
        "Cuando se tienen variables categóricas, ya no se habla de correlación o asociación lineal, sino de asociación o contingencia en términos de causa y efecto.\n",
        "\n",
        "El procedimiento estadístico habitual para contrastar la presencia de asociación en variables no numéricas es utilizar la prueba del chi-cuadrado (χ²) o la prueba exacta de Fisher en el caso de frecuencias pequeñas.\n",
        "\n",
        "Existen diferentes medidas para cuantificar la importancia de la asociación. Una de ellas es el Riesgo Relativo (RR), que utiliza el cociente de los riesgos en los dos grupos para indicar qué tan probable es que ocurra un suceso en el primer grupo en comparación con el segundo.\n",
        "\n",
        "Otra medida comúnmente utilizada es la \"odds ratio\" (OR), que representa la ventaja de un riesgo al comparar la frecuencia de un suceso con la frecuencia de su ausencia.\n",
        "\n",
        "Para realizar estas pruebas y calcular las medidas de asociación, se pueden utilizar herramientas estadísticas disponibles en paquetes como R.\n",
        "\n",
        "Al realizar el análisis bivariado, es importante considerar la significancia estadística de los resultados obtenidos para determinar si existe una asociación o no entre las variables estudiadas."
      ],
      "metadata": {
        "id": "rsu14EcAYhd-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Pruebas de hipótesis**\n",
        "\n",
        "Una prueba de hipótesis es un procedimiento estadístico que nos permite determinar si existe evidencia suficiente en los datos para rechazar una hipótesis nula (H0) en favor de una hipótesis alternativa (H1).\n",
        "\n",
        "### Hipótesis nula (H0)\n",
        "Es la hipótesis que queremos rechazar. Por ejemplo:\n",
        "\n",
        "* Para correlación: H0: r = 0 (no existe relación lineal)\n",
        "* Para Chi-cuadrado: H0: Las variables son independientes (no existe asociación)\n",
        "\n",
        "### Hipótesis alternativa (H1)\n",
        "Es la hipótesis que queremos aceptar. Por ejemplo:\n",
        "\n",
        "* H1: r ≠ 0 (existe relación lineal significativa)\n",
        "* H1: Las variables están asociadas (dependientes)\n",
        "\n",
        "### Nivel de significancia (α)\n",
        "Es la probabilidad de rechazar H0 cuando ésta es verdadera (error de tipo I). Usualmente α = 0.05.\n",
        "\n",
        "### P-valor\n",
        "Es la probabilidad de obtener un resultado igual o más extremo que el observado, si H0 fuera verdadera. Si p-valor < α, se rechaza H0. Si no, no hay evidencia suficiente para rechazar H0.\n",
        "\n",
        "### Decisión\n",
        "* Si p-valor < 0.05 (para α = 0.05), se rechaza H0 y se acepta H1. **Existe evidencia de asociación significativa.**\n",
        "* Si p-valor > 0.05, no se puede rechazar H0. **No hay evidencia suficiente de asociación significativa.**\n",
        "\n",
        "En resumen, las pruebas de hipótesis nos permiten determinar si existen diferencias significativas entre grupos o una relación significativa entre variables, mediante la comparación del p-valor con el nivel de significancia α."
      ],
      "metadata": {
        "id": "xJy4axUOdAPl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pruebas de Hipótesis en Análisis Bivariado\n",
        "\n",
        "Las pruebas de hipótesis son utilizadas en el análisis bivariado para evaluar la existencia de una asociación estadísticamente significativa entre dos variables. A continuación, se explica cómo realizar estas pruebas y los criterios de decisión asociados:\n",
        "\n",
        "### Variables Cuantitativas\n",
        "\n",
        "En el caso de tener dos variables cuantitativas, se utiliza el coeficiente de correlación de Pearson (r) para medir la asociación lineal. Para realizar la prueba de hipótesis, se plantean las siguientes hipótesis:\n",
        "\n",
        "- Hipótesis nula (H₀): No hay asociación lineal entre las variables en la población.\n",
        "- Hipótesis alternativa (H₁): Existe una asociación lineal entre las variables en la población.\n",
        "\n",
        "Para evaluar estas hipótesis, se realiza un análisis de significancia utilizando la prueba t de Student. Se calcula un valor p que representa la probabilidad de obtener una asociación igual o más extrema que la observada, asumiendo que la hipótesis nula es verdadera.\n",
        "\n",
        "Si el valor p es menor que un umbral predefinido (generalmente 0.05), se rechaza la hipótesis nula y se concluye que existe una asociación estadísticamente significativa entre las variables.\n",
        "\n",
        "### Variables Categóricas\n",
        "\n",
        "En el caso de tener variables categóricas, se utilizan pruebas como el chi-cuadrado (χ²) o la prueba exacta de Fisher para evaluar la asociación entre las variables. El procedimiento general es el siguiente:\n",
        "\n",
        "1. Se plantean las hipótesis:\n",
        "   - Hipótesis nula (H₀): No hay asociación entre las variables en la población.\n",
        "   - Hipótesis alternativa (H₁): Existe una asociación entre las variables en la población.\n",
        "\n",
        "2. Se calcula un estadístico de prueba (como el chi-cuadrado) que compara las frecuencias observadas con las frecuencias esperadas bajo la hipótesis nula.\n",
        "\n",
        "3. Se obtiene un valor p que representa la probabilidad de obtener una asociación igual o más extrema que la observada, asumiendo que la hipótesis nula es verdadera.\n",
        "\n",
        "4. Si el valor p es menor que el umbral predefinido (generalmente 0.05), se rechaza la hipótesis nula y se concluye que existe una asociación estadísticamente significativa entre las variables.\n",
        "\n",
        "Es importante tener en cuenta que el análisis bivariado puede incluir otras medidas de asociación, como el Riesgo Relativo (RR) o el odds ratio (OR). Estas medidas también se pueden someter a pruebas de hipótesis utilizando enfoques específicos.\n",
        "\n",
        "En resumen, las pruebas de hipótesis en el análisis bivariado permiten determinar si existe una asociación estadísticamente significativa entre dos variables. La decisión se basa en comparar el valor p obtenido con un umbral predefinido, generalmente 0.05. Si el valor p es menor que este umbral, se rechaza la hipótesis nula y se concluye que existe una asociación significativa."
      ],
      "metadata": {
        "id": "s6DuNNJPqyD9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KLl56JdIrlyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeUvlgFcktuU"
      },
      "source": [
        "# **3. Análisis de dos variable y el Cuarteto de Anscombe**\n",
        "\n",
        "La representación de datos es una tarea clave del análisis de datos. La utilización de una gráfica adecuada puede hacer que los resultados y conclusiones se comuniquen de una forma adecuada o no. Conocer y manejar diferentes herramientas es clave para poder seleccionar la gráfica adecua en cada ocasión. En esta entrada se va a repasar básicamente las funciones que ofrece la librería Seaborn.\n",
        "\n",
        "Este tema recorre una gran variedad de temas que pueden ser manipulados con el módulo seaborn para realizar un Análisis Exploratorio de los Datos (EDA).\n",
        "\n",
        "---\n",
        "\n",
        "# **CONTENIDOS**\n",
        "\n",
        ">3.1. El Cuarteto de Anscombe\n",
        ">3.2. Correlación y Regresión Lineal Simple\n",
        "    * Normalidad de y\n",
        "    * Coeficiente de Correlación de Pearson\n",
        "    * Coeficiente de Correlación de Spearman\n",
        "    * Coeficiente de Correlación Tau de kendall\n",
        ">3.3. Importancia de visualizar los datos (Paquetes matplotlib y seaborn)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFW3BGpzj_7r"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC2RT5f8anVZ"
      },
      "source": [
        "#### **BITACORA DE PROCEDIMIENTO**\n",
        "\n",
        "1. Confirmar la normalidad de la variable Y\n",
        "2. si **es normal**, entonces hago correlación de **pearson**\n",
        "3. No es normal entonces puedo evaluar la correlación LINEAL, con spearman o kendall\n",
        "\n",
        "LA VISUALIZACION DEBE CONFIRMAR TODO LO ANTERIOR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViYHCEf-dZW5"
      },
      "source": [
        "## **[3.1. El cuarteto de Anscombe](https://es.wikipedia.org/wiki/Cuarteto_de_Anscombe)**\n",
        "\n",
        "Cada conjunto consiste de once puntos (x, y) y fueron construidos por el estadístico F. J. Anscombe. Francis John \"Frank\" Anscombe (13 de mayo de 1918 - 17 de octubre de 2001) fue un estadístico inglés.\n",
        "\n",
        "El cuarteto de Anscombe comprende cuatro conjuntos de datos que tienen las mismas propiedades estadísticas, pero que evidentemente son distintas al inspeccionar sus gráficos respectivos.\n",
        "\n",
        "Un Ejercicio para ver la necesidad de realizar antes de cualquier análisis estadístico una exploración gráfica de los datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jwHykWdReuv"
      },
      "source": [
        "# Importamos los módulos necesarios\n",
        "import numpy as np   # Python Numérico, vectores y matrices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT_OHM3oDuEd"
      },
      "source": [
        "# seaborn se utiliza para visualización, es más sofisticado que matplotlib\n",
        "import seaborn as sns\n",
        "# https://seaborn.pydata.org/\n",
        "# https://seaborn.pydata.org/tutorial/regression.html\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir(sns)"
      ],
      "metadata": {
        "id": "W0_FTXmroWEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1luPV_4kyRBd"
      },
      "source": [
        "# Mirar los conjuntos de datos de seaborn\n",
        "sns.get_dataset_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQNVnvUkRpOG"
      },
      "source": [
        "import seaborn as sns\n",
        "# CUARTETO DE ANSCOMBE\n",
        "# La función load_dataset de seaborn permite cargar conjuntos de datos\n",
        "anscombe = sns.load_dataset(\"anscombe\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ruyy6zbcHajk"
      },
      "source": [
        "type(anscombe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewz2baTEzA8Z"
      },
      "source": [
        "anscombe.info() # Print a concise summary of a DataFrame."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbjt3LRWtaRp"
      },
      "source": [
        "anscombe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3sK-oc5zohk"
      },
      "source": [
        "anscombe.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myrXN2_cz30D"
      },
      "source": [
        "anscombe.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFjMN67Cz9pS"
      },
      "source": [
        "anscombe.tail(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0V74dZYTkTD"
      },
      "source": [
        "# La función query() de seaborn permite extraer un subconjunto del dataset original\n",
        "a1 = anscombe.query(\"dataset == 'I' \")  # Query the columns of a DataFrame with a boolean expression."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAMDGRzwtxGH"
      },
      "source": [
        "a1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2r-VRyW0Q8_"
      },
      "source": [
        "a1.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0B2QcmIT56_"
      },
      "source": [
        "# Extraer el dataset == II\n",
        "a2 = anscombe.query(\"dataset == 'II' \")\n",
        "a2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WAMmzneLhvx"
      },
      "source": [
        "# Extraer el dataset == III\n",
        "a3 = anscombe.query(\"dataset == 'III' \")\n",
        "a3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUtU9_k0LvoU"
      },
      "source": [
        "# Extraer el dataset == IV\n",
        "a4 = anscombe.query(\"dataset == 'IV' \")\n",
        "a4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkZ-HfdnMKcJ"
      },
      "source": [
        "a1.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibtuL8M-MJlm"
      },
      "source": [
        "a2.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxbyGycwMJaA"
      },
      "source": [
        "a3.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8fSzZhUMJK_"
      },
      "source": [
        "a4.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m79-T1HTVxoM"
      },
      "source": [
        "## **3.2. Correlación y Regresión Lineal Simple**\n",
        "\n",
        "La correlación lineal y la regresión lineal simple son métodos estadísticos que estudian la relación lineal existente entre dos variables. Antes de profundizar en cada uno de ellos, conviene destacar algunas diferencias:\n",
        "\n",
        "1. La correlación cuantifica como de relacionadas están dos variables, mientras que la regresión lineal consiste en generar una ecuación (**modelo**) que, basándose en la relación existente entre ambas variables, permita **predecir** el valor de una a partir de la otra.\n",
        "\n",
        "2. El cálculo de la correlación entre dos variables es independiente del orden o asignación de cada variable a X e Y, mide únicamente la relación entre ambas **sin considerar dependencias**. En el caso de la regresión lineal, el modelo varía según qué variable se considere dependiente de la otra (**lo cual no implica causa-efecto**).\n",
        "\n",
        "3. A nivel experimental, la correlación se suele emplear cuando ninguna de las variables se ha controlado, simplemente se han medido ambas y se desea saber si están relacionadas. En el caso de estudios de regresión lineal, es más común que una de las variables se controle (tiempo, concentración de reactivo, temperatura…) y se mida la otra.\n",
        "\n",
        "4. Por norma general, los estudios de correlación lineal preceden a la generación de modelos de regresión lineal. Primero se analiza si ambas variables están correlacionadas y, en caso de estarlo, se procede a generar el modelo de regresión.\n",
        "\n",
        "* Puede ampliar conceptos en este enlace: [Correlación](https://es.wikipedia.org/wiki/Correlaci%C3%B3n)\n",
        "\n",
        "* También en este otro enlace: [El coeficiente de correlación de Pearson (con ejemplo en Python)](https://medium.com/@hdezfloresmiguelangel/el-coeficiente-de-correlaci%C3%B3n-de-pearson-con-ejemplo-en-python-6e8588f67e35)\n",
        "\n",
        "[Coeficiente de correlación de Pearson](https://es.wikipedia.org/wiki/Coeficiente_de_correlaci%C3%B3n_de_Pearson)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aplicaciones en Ingeniería\n",
        "\n",
        "Algunas áreas de la ingeniería en las que se pueden aplicar los temas de correlación lineal y no lineal de variables, y las regresiones lineales y no lineales, así como las regresiones múltiples:\n",
        "\n",
        "1. **Análisis de estructuras**: En el diseño y análisis de estructuras, es común utilizar la regresión lineal y no lineal para determinar la relación entre variables como la carga y la deformación.\n",
        "\n",
        "2. **Hidrología**: En hidrología, se utilizan técnicas de regresión para analizar la relación entre variables como la precipitación y el caudal de un río.\n",
        "\n",
        "3. **Geotecnia**: En geotecnia, se pueden utilizar técnicas de regresión para analizar la relación entre variables como la densidad del suelo y su resistencia a la compresión.\n",
        "\n",
        "4. **Transporte**: En el análisis del transporte, se pueden utilizar técnicas de regresión para analizar la relación entre variables como la velocidad y el flujo de tráfico.\n",
        "\n",
        "5. **Gestión de proyectos**: En la gestión de proyectos, se pueden utilizar técnicas de regresión para analizar la relación entre variables como el tiempo y el costo de un proyecto.\n",
        "\n",
        "6. **Sismología**: En sismología, se pueden utilizar técnicas de regresión para analizar la relación entre variables como la magnitud y la frecuencia de los terremotos.\n",
        "\n",
        "7. **Diseño de experimentos en agricultura y agroindustria**: La correlación lineal y no lineal de variables, y las regresiones lineales y no lineales, son herramientas importantes para diseñar experimentos agrícolas y de procesamiento de alimentos, ya que permiten identificar la relación entre diferentes variables y cómo afectan el resultado final del experimento.\n",
        "\n",
        "8. **Optimización de procesos en la industria alimentaria**: Las regresiones múltiples pueden ser utilizadas para optimizar los procesos de producción en la industria alimentaria, identificando las variables que más afectan el resultado final del proceso y ajustando las condiciones de producción para obtener el mejor resultado posible.\n",
        "\n",
        "En resumen, la regresión lineal y no lineal y la correlación de variables son herramientas útiles en muchos campos de la ingeniería para analizar y comprender la relación entre variables y para hacer predicciones en función de esas relaciones."
      ],
      "metadata": {
        "id": "Fn2Oo_7xYEpg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4G2Wca9UyX5"
      },
      "source": [
        "# La función corr() de pandas permite calcular el coeficiente de correlación\n",
        "a1.corr(method=\"pearson\")       # Compute pairwise correlation of columns, excluding NA/null values."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXe2h98G7KZg"
      },
      "source": [
        "# Podemos pedir ayuda de la función corr de pandas\n",
        "a1.corr?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRFe7CHfVNCF"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Graficamos el par x,y del conjunto de datos a1\n",
        "plt.plot(a1.x,a1.y,\"b*\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gy1pMwhIO694"
      },
      "source": [
        "# calculamos el coeficiente de correlación de pearson entre x,y de a2\n",
        "a2.corr(method=\"pearson\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0RzmmFGPJfS"
      },
      "source": [
        "# Graficamos el par x,y de a2\n",
        "plt.plot(a2.x,a2.y,\"bo\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eIb2ir_2oc4"
      },
      "source": [
        "# calculamos el coeficiente de correlación de spearman entre x,y de a2\n",
        "a2.corr(method=\"spearman\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IggwDID262R"
      },
      "source": [
        "# calculamos el coeficiente de correlación de kendall entre x,y de a2\n",
        "a2.corr(method=\"kendall\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sx7ftj9FPj8d"
      },
      "source": [
        "# calculamos el coeficiente de correlación de pearson entre x,y de a3\n",
        "a3.corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn4hnUj5Pq1D"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Graficamos el par x,y de a3\n",
        "plt.plot(a3.x,a3.y,\"bo\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ntHva3b3cTn"
      },
      "source": [
        "# calculamos el coeficiente de correlación de spearman entre x,y de a2\n",
        "a3.corr(method=\"spearman\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5nmMpSfibxR"
      },
      "source": [
        "a3.corr(method='kendall')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkBpKn3hQTET"
      },
      "source": [
        "# calculamos el coeficiente de correlación pearson entre x,y de a4\n",
        "a4.corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Meu5TuyrQYTw"
      },
      "source": [
        "# Graficamos el par x,y d a4\n",
        "plt.plot(a4.x,a4.y,\"ro\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inH8pL8lQ3Sy"
      },
      "source": [
        "# calculamos el coeficiente de correlación spearman entre x,y de a4\n",
        "a4.corr(method=\"spearman\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fC1QjM7RjfB"
      },
      "source": [
        "# calculamos el coeficiente de correlación kenall entre x,y de a4\n",
        "a4.corr(method=\"kendall\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbf4qXyISZtm"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bUgCwVrYqvB"
      },
      "source": [
        "# *Coeficiente de correlación de Pearson y valor p para probar la no correlación*.\n",
        "\n",
        "---\n",
        "\n",
        "[scipy.stats.pearsonr(x , y)](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html#r8c6348c62346-3)\n",
        "\n",
        "---\n",
        "\n",
        "El [coeficiente de correlación de Pearson](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) mide la relación lineal entre dos conjuntos de datos.\n",
        "\n",
        "El cálculo del valor p se basa en el supuesto de que cada conjunto de datos se distribuye normalmente. Como otros coeficientes de correlación, éste varía entre -1 y +1, donde 0 implica que no hay correlación. Las correlaciones de -1 o +1 implican una relación lineal exacta. Las correlaciones positivas implican que a medida que aumenta x, también lo hace y. Las correlaciones negativas implican que a medida que x aumenta, y disminuye.\n",
        "\n",
        "El valor p indica aproximadamente la probabilidad de que un sistema no correlacionado produzca conjuntos de datos que tengan una correlación de Pearson al menos tan extrema como la calculada a partir de estos conjuntos de datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTasN9OWWn82"
      },
      "source": [
        "from scipy import stats\n",
        "r, p = stats.pearsonr(a1.x, a1.y)\n",
        "# H0: no hay correlacion\n",
        "if p < 0.05:\n",
        "    print(p, \" SI Hay Correlación Lineal\")\n",
        "elif p > 0.05:\n",
        "    print(p, \" NO Hay Correlación Lineal\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dp5qkA8Akte8"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09aec87MkWF3"
      },
      "source": [
        "# anscombe = sns.load_dataset('anscombe')\n",
        "# a1 = anscombe.query(\"dataset == 'I'\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(a1.x,a1.y,'r*')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KHB-jZjyhYX"
      },
      "source": [
        "from scipy import stats\n",
        "stats.pearsonr(a2.x, a2.y)\n",
        "\n",
        "# H0: no hay correlacion\n",
        "if p < 0.05:\n",
        "    print(p, \" SI Hay Correlación Lineal\")\n",
        "elif p > 0.05:\n",
        "    print(p, \" NO Hay Correlación Lineal\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6YNKgdMlfdz"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(a2.x,a2.y,'r*')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaXauTQJyh1S"
      },
      "source": [
        "from scipy import stats\n",
        "stats.pearsonr(a3.x, a3.y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSUrr8kDly6X"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(a3.x,a3.y,'r*')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM_8DLOHyhBO"
      },
      "source": [
        "from scipy import stats\n",
        "stats.pearsonr(a4.x, a4.y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfXZ-zLPcrSD"
      },
      "source": [
        "# **Coeficiente de Correlación de Spearman con el valor p asociado**.\n",
        "\n",
        "---\n",
        "[scipy.stats.spearmanr( a , b = Ninguno , eje = 0 , nan_policy = 'propagar' )](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html#scipy.stats.spearmanr)\n",
        "---\n",
        "\n",
        "El coeficiente de correlación de rango y orden de Spearman es una medida no paramétrica de la monotonicidad de la relación entre dos conjuntos de datos. A diferencia de la correlación de Pearson, **la correlación de Spearman no supone que ambos conjuntos de datos estén distribuidos normalmente**. Como otros coeficientes de correlación, éste varía entre -1 y +1, donde 0 implica que no hay correlación. Las correlaciones de -1 o +1 implican una relación monótona exacta. Las correlaciones positivas implican que a medida que aumenta x, también lo hace y. Las correlaciones negativas implican que a medida que x aumenta, y disminuye.\n",
        "\n",
        "El valor p indica aproximadamente la probabilidad de que un sistema no correlacionado produzca conjuntos de datos que tengan una correlación de Spearman al menos tan extrema como la calculada a partir de estos conjuntos de datos. Los valores p no son del todo fiables, pero probablemente sean razonables para conjuntos de datos superiores a 500 o más."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUDd5io2XEPY"
      },
      "source": [
        "from scipy import stats\n",
        "stats.spearmanr(a1.x,a1.y)\n",
        "\n",
        "# Supon no normalidad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5n_KDTvYzOna"
      },
      "source": [
        "from scipy import stats\n",
        "stats.spearmanr(a2.x,a2.y)\n",
        "\n",
        "# Supon no normalidad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcvjUJcZzO1M"
      },
      "source": [
        "from scipy import stats\n",
        "stats.spearmanr(a3.x,a3.y)\n",
        "\n",
        "# Supon no normalidad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrlTluIOzOaq"
      },
      "source": [
        "from scipy import stats\n",
        "stats.spearmanr(a4.x,a4.y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ2tnROWdiUC"
      },
      "source": [
        "# **tau de Kendall, una medida de correlación para datos ordinales**.\n",
        "\n",
        "La [tau de Kendall](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kendalltau.html#r4cd1899fa369-1) es una medida de la correspondencia entre dos clasificaciones. Los valores cercanos a 1 indican un fuerte acuerdo, los valores cercanos a -1 indican un fuerte desacuerdo.\n",
        "\n",
        "Esta es la versión “tau-b” de 1945 del tau de Kendall [1] , que puede explicar los vínculos y que se reduce a la versión “tau-a” de 1938 [2] en ausencia de vínculos.\n",
        "\n",
        "[1] Maurice G. Kendall, “El tratamiento de los vínculos en los problemas de clasificación”, Biometrika Vol. 33, núm. 3, págs. 239-251. 1945\n",
        "\n",
        "[2] Maurice G. Kendall, \"Una nueva medida de correlación de rango\", Biometrika Vol. 30, núm. 1/2, págs. 81-93, 1938."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HyvL8heXSsw"
      },
      "source": [
        "from scipy import stats\n",
        "stats.kendalltau(a1.x, a1.y)\n",
        "\n",
        "# Supon no normalidad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDdbFDihz2Td"
      },
      "source": [
        "from scipy import stats\n",
        "stats.kendalltau(a2.x, a2.y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBmJ7KZXz2_0"
      },
      "source": [
        "from scipy import stats\n",
        "stats.kendalltau(a3.x, a3.y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXlWXo45z17G"
      },
      "source": [
        "from scipy import stats\n",
        "stats.kendalltau(a4.x, a4.y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWVIATlPKntK"
      },
      "source": [
        "#### PRUEBAS DE NORMALIDAD DE LOS DATOS\n",
        "\n",
        "---\n",
        "\n",
        "[Distribución normal](https://es.wikipedia.org/wiki/Distribuci%C3%B3n_normal)\n",
        "\n",
        "---\n",
        "\n",
        "[Test de Shapiro–Wilk](https://es.wikipedia.org/wiki/Test_de_Shapiro%E2%80%93Wilk)\n",
        "\n",
        "---\n",
        "\n",
        "[Prueba de Kolmogorov-Smirnov](https://es.wikipedia.org/wiki/Prueba_de_Kolmogorov-Smirnov)\n",
        "\n",
        "---\n",
        "\n",
        "[Prueba de Anderson-Darling](https://es.wikipedia.org/wiki/Prueba_de_Anderson-Darling)\n",
        "\n",
        "---\n",
        "\n",
        "Comparación De Pruebas De Normalidad\n",
        "---\n",
        "[Comparación De Pruebas De Normalidad](http://gfnun.unal.edu.co/fileadmin/content/eventos/simposioestadistica/documentos/memorias/MEMORIAS_2015/Comunicaciones/Est_Matematica/Isaza_Acevedo___Hernandez_Pruebas_Normalidad.pdf)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjhD23_-Xl18"
      },
      "source": [
        "### [scipy.stats.normaltest](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.normaltest.html) para probar la normalidad\n",
        "\n",
        "---\n",
        "\n",
        "[Normality Tests in Python/v3](https://plot.ly/python/v3/normality-test/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lu9JKn-xSXHk"
      },
      "source": [
        "import scipy.stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LvrcpmWSapw"
      },
      "source": [
        "dir(scipy.stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj2GUr-xTPk-"
      },
      "source": [
        "from scipy.stats import shapiro"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMZve6APTkgd"
      },
      "source": [
        "stat, p = shapiro(a1.y)\n",
        "# Shapiro-Wilk Test\n",
        "# H0: SI ES NORMAL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dq7MKUlUW6-"
      },
      "source": [
        "stat, p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHFyMkiw4oee"
      },
      "source": [
        "Podemos concluir que la variable Y de a1, se distribuye normalmente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lyy4m-nWzSf"
      },
      "source": [
        "# https://plot.ly/python/v3/normality-test/\n",
        "from scipy.stats import shapiro\n",
        "stat, p = shapiro(a1.y)\n",
        "stat,p\n",
        "# Shapiro-Wilk Test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7gXQUucYxZW"
      },
      "source": [
        "# Anderson-Darling Test\n",
        "from scipy.stats import anderson\n",
        "anderson(a1.y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pSBUIQqZozX"
      },
      "source": [
        "# D'Agostino's K2 Test\n",
        "from scipy.stats import normaltest\n",
        "\n",
        "stat, p = normaltest(a1.y)\n",
        "stat,p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJIEHC00aOuV"
      },
      "source": [
        "# Prueba de shapiro-wilk para el vector y del dataset II\n",
        "from scipy.stats import shapiro\n",
        "stat, p = shapiro(a2.y)\n",
        "stat,p\n",
        "\n",
        "if p > 0.05:\n",
        "    print(p, \"SI ES normal\")\n",
        "elif p < 0.05:\n",
        "    print(p, \"NO ES normal\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqpRyaFwVdb6"
      },
      "source": [
        "# Prueba de shapiro-wilk para el vector y del dataset III\n",
        "from scipy.stats import shapiro\n",
        "stat, p = shapiro(a3.y)\n",
        "stat, p\n",
        "if p > 0.05:\n",
        "    print(p, \"SI ES normal\")\n",
        "elif p < 0.05:\n",
        "    print(p, \"NO ES normal\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xItbbUudVq8L"
      },
      "source": [
        "# Prueba de shapiro-wilk para el vector y del dataset IV\n",
        "from scipy.stats import shapiro\n",
        "stat, p = shapiro(a4.y)\n",
        "stat, p\n",
        "if p > 0.05:\n",
        "    print(p, \"SI ES normal\")\n",
        "elif p < 0.05:\n",
        "    print(p, \"NO ES normal\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTNNWODFpODt"
      },
      "source": [
        "RESUMIENDO:\n",
        "\n",
        "1. NORMALIDAD\n",
        "2. SI HAY NORMALIDAD, PRUEBO LA CORRELACION LINEAL CON PEARSON\n",
        "3. SI NO HAY NORMALIDAD, PRUEBO LA CORELACIÓN LINEAL CON SPEARMAN O KENDALL\n",
        "\n",
        "PERO DEBO CONFIRMAR CON LA GRAFICA"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EWScDhbYrta8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWCLpr8I_SvP"
      },
      "source": [
        "## **3.3. Importancia de visualizar los datos (Paquetes matplotlib y seaborn)**.\n",
        "\n",
        "---\n",
        "\n",
        "[Visualizing statistical relationships: Visualizando relaciones estadísticas](https://seaborn.pydata.org/tutorial/relational.html#relational-tutorial)\n",
        "\n",
        "---\n",
        "\n",
        "El análisis estadístico es un proceso para comprender cómo las variables en un conjunto de datos se relacionan entre sí y cómo esas relaciones dependen de otras variables. La visualización puede ser un componente central de este proceso porque, cuando los datos se visualizan correctamente, el sistema visual humano puede ver tendencias y patrones que indican una relación.\n",
        "\n",
        "Discutiremos tres funciones nacidas en este tutorial. El que más usaremos es **relplot()**. Esta es una función de nivel de figura para visualizar relaciones estadísticas usando dos enfoques comunes: gráficos de dispersión y gráficos de líneas. **relplot()** combina a **FacetGrid** con una de las dos siguientes funciones de nivel de ejes:\n",
        "\n",
        "---\n",
        "\n",
        "+ scatterplot()(con kind=\"scatter\"; el valor predeterminado)\n",
        "+ lineplot()(con kind=\"line\")\n",
        "\n",
        "---\n",
        "\n",
        "Como veremos, estas funciones pueden ser bastante esclarecedoras porque utilizan representaciones de datos simples y fáciles de entender que, sin embargo, pueden representar estructuras complejas de conjuntos de datos. Pueden hacerlo porque trazan gráficos bidimensionales que pueden mejorarse mapeando hasta tres variables adicionales utilizando la semántica de matiz, tamaño y estilo.\n",
        "\n",
        "---\n",
        "\n",
        "##función relplot()\n",
        "\n",
        "---\n",
        "\n",
        "seaborn.relplot\n",
        "\n",
        "---\n",
        "\n",
        "**seaborn.relplot**(x=None, y=None, hue=None, size=None, style=None, data=None, row=None, col=None, col_wrap=None, row_order=None, col_order=None, palette=None, hue_order=None, hue_norm=None, sizes=None, size_order=None, size_norm=None, markers=None, dashes=None, style_order=None, legend='brief', kind='scatter', height=5, aspect=1, facet_kws=None, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GX9JRF8vVwo1"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "ans = sns.load_dataset(\"anscombe\")\n",
        "\n",
        "sns.relplot(x=\"x\",y=\"y\",data=ans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNzuDzldDd6l"
      },
      "source": [
        "\"\"\"\n",
        "Si bien los puntos se trazan en dos dimensiones, se puede agregar otra dimensión al\n",
        "diagrama coloreando los puntos de acuerdo con una tercera variable.\n",
        "En seaborn, esto se conoce como el uso de un \"tono semántico\" (\"hue semantic\"),\n",
        "porque el color del punto adquiere significado:\n",
        "\"\"\"\n",
        "sns.relplot(x=\"x\",y=\"y\",hue=\"dataset\", data=ans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJbqXOKcEbOL"
      },
      "source": [
        "# Visualizando cada par x,y, utilizando el argumento query\n",
        "sns.relplot(x=\"x\",y=\"y\",data=ans.query(\"dataset=='I'\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvVXfOyM6e6J"
      },
      "source": [
        "# la función regplot() Grafica el dato y Ajusta el dato a un modelo de regresión lineal\n",
        "sns.regplot(x=\"x\", y=\"y\",data=ans.query(\"dataset == 'I' \"  ))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FcH86WDGJmv"
      },
      "source": [
        "sns.relplot(x=\"x\",y=\"y\",data=ans.query(\"dataset=='II'\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csOeEzvI8HvH"
      },
      "source": [
        "sns.regplot(x=\"x\",y=\"y\",data=ans.query(\"dataset=='II'\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gW8xzVW8bvH"
      },
      "source": [
        "sns.regplot(x=\"x\",y=\"y\",data=ans.query(\"dataset=='II'\"), order=2)\n",
        "# Realiza un ajuste de regresión polinomial de orden 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0OGd2nLGLqB"
      },
      "source": [
        "sns.relplot(x=\"x\",y=\"y\",data=ans.query(\"dataset=='III'\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPMZ01_1r5l-"
      },
      "source": [
        "sns.regplot(x=\"x\",y=\"y\",data=ans.query(\"dataset=='III'\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HekoQky49nwe"
      },
      "source": [
        "sns.regplot(x=\"x\",y=\"y\",data=ans.query(\"dataset=='III'\"),robust=True,ci=None)\n",
        "# Se hace regresión robusta cuando se tienen outlier que no se quieren eliminar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f-cvp2fMltB"
      },
      "source": [
        "[Regresión Robusta en Wikipedia](https://es.wikipedia.org/wiki/Regresi%C3%B3n_robusta)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1IgZHdKeIT5"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.get_dataset_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eXefiPL9-Sf"
      },
      "source": [
        "import seaborn as sns\n",
        "ans = sns.load_dataset(\"anscombe\")\n",
        "sns.lmplot(x=\"x\",y=\"y\",data=ans.query(\"dataset=='III'\"),robust=True,ci=None)\n",
        "# Regresión Robusta: https://es.wikipedia.org/wiki/Regresi%C3%B3n_robusta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sMIp7BpFg_O"
      },
      "source": [
        "sns.relplot(x=\"x\",y=\"y\",data=ans.query(\"dataset=='IV'\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNrMJ3EdM9zz"
      },
      "source": [
        "sns.regplot(x=\"x\",y=\"y\",data=ans.query(\"dataset=='IV' \"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EO8912DM-Kt"
      },
      "source": [
        "sns.lmplot(x=\"x\",y=\"y\",data=ans.query(\"dataset=='IV'\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jahEA42M-ek"
      },
      "source": [
        "sns.regplot(x=\"x\",y=\"y\",data=ans.query(\"dataset=='IV'\"),robust=True,ci=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SphZl6jCGNFh"
      },
      "source": [
        "Ejercicio:\n",
        "Realice el tutorial de:\n",
        "[Visualizing linear relationships: Visualizando relaciones lineales](https://seaborn.pydata.org/tutorial/regression.html#regression-tutorial)\n",
        "\n",
        "---\n",
        "\n",
        "Visualizando relaciones lineales\n",
        "Muchos conjuntos de datos contienen múltiples variables cuantitativas, y el objetivo de un análisis a menudo es relacionar esas variables entre sí. Nosotros previamente discutido funciones que pueden lograr esto mostrando la distribución conjunta de dos variables. Sin embargo, puede ser muy útil utilizar modelos estadísticos para estimar una relación simple entre dos conjuntos ruidosos de observaciones. Las funciones discutidas en este capítulo lo harán a través del marco común de regresión lineal.\n",
        "\n",
        "En el espíritu de Tukey, las gráficas de regresión en seaborn están destinadas principalmente a agregar una guía visual que ayuda a enfatizar los patrones en un conjunto de datos durante los análisis exploratorios de datos. Es decir que **seaborn no es en sí mismo un paquete para el análisis estadístico**.\n",
        "\n",
        "**Para obtener medidas cuantitativas relacionadas con el ajuste de los modelos de regresión, debe usar** **statsmodels**. Sin embargo, el objetivo de seaborn es hacer que la exploración de un conjunto de datos a través de la visualización sea rápida y fácil, ya que hacerlo es tan importante (si no más) que explorar un conjunto de datos a través de tablas de estadísticas.\n",
        "\n",
        "---\n",
        "\n",
        "Funciones para dibujar modelos de regresión lineal\n",
        "\n",
        "---\n",
        "\n",
        "Se utilizan dos funciones principales en seaborn para visualizar una relación lineal determinada mediante regresión. Estas funciones, **regplot()** y **lmplot()** están estrechamente relacionadas, y comparten gran parte de su funcionalidad principal. Sin embargo, es importante comprender las formas en que difieren, para que pueda elegir rápidamente la herramienta correcta para un trabajo en particular.\n",
        "\n",
        "En la invocación simple, ambas funciones dibujan un diagrama de dispersión de dos variables, x y y, entoncers se ajustan al modelo de regresión y~x, se grafica la línea de regresión resultante y un intervalo de confianza del 95% para esa regresión."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pgg1O07QNlh8"
      },
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set(color_codes=True)\n",
        "# Carga el conjunto de datos tips que son las propinas de un restaunte\n",
        "tips = sns.load_dataset(\"tips\")\n",
        "tips"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShrXY8IKQK80"
      },
      "source": [
        "propinas = tips"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfzTinaQQRnx"
      },
      "source": [
        "propinas.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQWcgcXFQSC8"
      },
      "source": [
        "propinas.columns\n",
        "# La propiedad columns muestra los nombres de las columnas, pero también se\n",
        "# utiliza para cambiar los nombres de las columnas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuoLPTrpQSXP"
      },
      "source": [
        "columnas = [\"cuenta\",\"propina\",\"sexo\",\"fuma\",\"dia\",\"tiempo\",\"personas\"]\n",
        "propinas.columns = columnas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVkIcWe8QMeL"
      },
      "source": [
        "propinas.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLsDSug3QMuJ"
      },
      "source": [
        "propinas.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bKFl0ONNlZ7"
      },
      "source": [
        "\"\"\"\n",
        "En la forma mas simple, ambas funciones dibujan un diagrama de dispersión de las\n",
        "dos variables, x y y, a continuación, se ajustan el modelo de regresión\n",
        "y la línea de regresión resultante y un intervalo de confianza\n",
        "del 95% para la regresión y ~ x\n",
        "\"\"\"\n",
        "sns.regplot(x=\"cuenta\", y=\"propina\", data=propinas);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ36JWVsO-EC"
      },
      "source": [
        "# Lo mismo utilizando la función lmplot()\n",
        "sns.lmplot(x=\"cuenta\", y=\"propina\", data=propinas);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaFM73cgPdxV"
      },
      "source": [
        "You should note that the resulting plots are identical, except that the figure shapes are different. We will explain why this is shortly. For now, the other main difference to know about is that regplot() accepts the x and y variables in a variety of formats including simple numpy arrays, pandas Series objects, or as references to variables in a pandas DataFrame object passed to data. In contrast, lmplot() has data as a required parameter and the x and y variables must be specified as strings. This data format is called “long-form” or “tidy” data. Other than this input flexibility, regplot() possesses a subset of lmplot()’s features, so we will demonstrate them using the latter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yV0s-GZOO954"
      },
      "source": [
        "\"\"\"\n",
        "Es posible ajustar una regresión lineal cuando una de las variables toma valores\n",
        "discretos, sin embargo, el diagrama de dispersión simple producido por este\n",
        "tipo de conjunto de datos a menudo no es óptimo:\n",
        "\"\"\"\n",
        "sns.lmplot(x=\"personas\", y=\"propina\", data=propinas);\n",
        "# size es el número de personas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxzCWDs0O9qN"
      },
      "source": [
        "\"\"\"\n",
        "Una opción es agregar algo de ruido aleatorio (\"jitter\") a los valores discretos\n",
        "para que la distribución de esos valores sea más clara. Tenga en cuenta que\n",
        "la fluctuación de fase se aplica solo a los datos del diagrama de dispersión\n",
        "y no influye en el ajuste de la línea de regresión:\n",
        "\"\"\"\n",
        "sns.lmplot(x=\"personas\", y=\"propina\", data=propinas, x_jitter=.1);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQaQp4toNlS2"
      },
      "source": [
        "\"\"\"\n",
        "Una segunda opción es colapsar sobre las observaciones en cada contenedor\n",
        "discreto para trazar una estimación de tendencia central junto con un intervalo\n",
        "de confianza:\n",
        "\"\"\"\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "sns.lmplot(x=\"personas\", y=\"propina\", data=propinas, x_estimator=np.mean);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IP3IG6ZaQk3_"
      },
      "source": [
        "#### Adaptación de diferentes tipos de modelos\n",
        "\n",
        "---\n",
        "\n",
        "El modelo de regresión lineal simple utilizado anteriormente es muy sencillo de ajustar, sin embargo, no es apropiado para algunos tipos de conjuntos de datos. El conjunto de datos del cuarteto de Anscombe muestra algunos ejemplos en los que la regresión lineal simple proporciona una estimación idéntica de una relación en la que la inspección visual simple muestra claramente las diferencias. Por ejemplo, en el primer caso, la regresión lineal es un buen modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRUvAJaHNlLF"
      },
      "source": [
        "anscombe = sns.load_dataset(\"anscombe\")\n",
        "\n",
        "sns.lmplot(x=\"x\", y=\"y\", data=anscombe.query(\"dataset == 'I'\"),\n",
        "           ci=None, scatter_kws={\"s\": 80});"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLLyHllQQfap"
      },
      "source": [
        "# La relación lineal en el segundo conjunto de datos es la misma,\n",
        "# pero el gráfico muestra claramente que este no es un buen modelo:\n",
        "\n",
        "sns.lmplot(x=\"x\", y=\"y\", data=anscombe.query(\"dataset == 'II'\"),\n",
        "           ci=None, scatter_kws={\"s\": 80});"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWzBv0sbQfOO"
      },
      "source": [
        "# En presencia de este tipo de relaciones de orden superior,\n",
        "# lmplot() y regplot() puede ajustarse a un modelo de regresión polinómica\n",
        "# para explorar tipos simples de tendencias no lineales en el conjunto de datos:\n",
        "\n",
        "sns.lmplot(x=\"x\", y=\"y\", data=anscombe.query(\"dataset == 'II'\"),\n",
        "           order=2, ci=None, scatter_kws={\"s\": 80});\n",
        "\n",
        "# Observe el argumento order=2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_VWAqf-QfAm"
      },
      "source": [
        "# Un problema diferente lo plantean las observaciones \"atípicas\" que se\n",
        "# desvían por alguna otra razón que no sea la relación principal en estudio:\n",
        "\n",
        "sns.lmplot(x=\"x\", y=\"y\", data=anscombe.query(\"dataset == 'III'\"),\n",
        "           ci=None, scatter_kws={\"s\": 80});"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bydyAoJsQeyw"
      },
      "source": [
        "# En presencia de valores atípicos, puede ser útil ajustar una regresión robusta,\n",
        "# que utiliza una función de pérdida diferente para reducir los residuos\n",
        "# relativamente grandes:\n",
        "\n",
        "sns.lmplot(x=\"x\", y=\"y\", data=anscombe.query(\"dataset == 'III'\"),\n",
        "           robust=True, ci=None, scatter_kws={\"s\": 80});"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3HPDIikapib"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTMqGJuEimJ6"
      },
      "source": [
        "#### MODELACIONES\n",
        "1. Predicción\n",
        "2. Clasificación\n",
        "3. Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1CLXh2RatIA"
      },
      "source": [
        "# **Variables binaria y regresión Logística**\n",
        "\n",
        "[Regresión Logística](https://es.wikipedia.org/wiki/Regresi%C3%B3n_log%C3%ADstica)\n",
        "\n",
        "Videos sobre la Justificación de la Regresión Logística:\n",
        "\n",
        "[Regresion Logistica: Introduccion](https://www.youtube.com/watch?v=gcvml4gwyrg&list=RDCMUCbdsyFBdG0Fmjroqk-ZoPcg&start_radio=1&t=33)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paXBO1_PWSwh"
      },
      "source": [
        "propinas.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ut1wW6BWwkA"
      },
      "source": [
        "propinon = propinas.propina / propinas.cuenta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xg22zMoYXDlK"
      },
      "source": [
        "propinon"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQiVEh6tXD1p"
      },
      "source": [
        "# Creando una variable dicotomica (binaria)\n",
        "propinon = (propinas.propina / propinas.cuenta)   > 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91OJtJD1XFTz"
      },
      "source": [
        "propinon.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E-AJbrGXFqF"
      },
      "source": [
        "propinas[\"propinon\"] = (propinas.propina / propinas.cuenta) > 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5rE3ubeXF9x"
      },
      "source": [
        "propinas.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDTI8tfINlBt"
      },
      "source": [
        "# Cuando la variable es binaria, la regresión lineal simple también \"funciona\"\n",
        "# pero proporciona predicciones inverosímiles:\n",
        "import seaborn as sns\n",
        "tips = sns.load_dataset(\"tips\")\n",
        "\n",
        "tips[\"big_tip\"] = (tips.tip / tips.total_bill) > .15\n",
        "\n",
        "sns.lmplot(x=\"total_bill\", y=\"big_tip\", data=tips,\n",
        "           y_jitter=.03);\n",
        "\n",
        "# Observe la definición de una nueva variable llamada big_tip, que se agrega\n",
        "# al conjunto de datos tips. Se ve claramente que es una proporción de las\n",
        "# propinas (tip) sobre la cuenta total (total_bill), pero categoriza a las que\n",
        "# son mayores al 15% (> 0.15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vy9kau_YNk0N"
      },
      "source": [
        "# Observe que la nueva variable es una variable dicotómica con valores\n",
        "# True aquellas proporciones mayores de 15%\n",
        "tips.big_tip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOW8xiCpTe6E"
      },
      "source": [
        "# La solución en este caso es ajustar una regresión logística, de modo que la\n",
        "# línea de regresión muestre la probabilidad estimada de un valor dado de\n",
        "# y = 1 para un valor dado de x\n",
        "sns.lmplot(x=\"total_bill\", y=\"big_tip\", data=tips,\n",
        "           logistic=True, y_jitter=.03);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deUnktRlQMLs"
      },
      "source": [
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSsPqc4rQLmg"
      },
      "source": [
        "p = sns.load_dataset(\"tips\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkPwqviSQLJW"
      },
      "source": [
        "p.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kElkZ5RQKt0"
      },
      "source": [
        "p.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gVPuUvDRQM2"
      },
      "source": [
        "columnas = (\"cuenta\",\"propina\",\"sexo\",\"fuma\",\"dia\",\"hora\",\"personas\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0gzxap1RPzp"
      },
      "source": [
        "p.columns = columnas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1PVTDc3RPbD"
      },
      "source": [
        "p.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkP4Z9RzRO_g"
      },
      "source": [
        "sns.relplot(x=\"cuenta\",y=\"propina\",data=p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCAaYUTRmNP0"
      },
      "source": [
        "sns.relplot(x=\"cuenta\",y=\"propina\", hue=\"sexo\",data=p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAG8nRSYVF3T"
      },
      "source": [
        "sns.relplot(x=\"cuenta\",y=\"propina\", hue=\"sexo\",col=\"hora\",data=p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IN_i4Ff5VFjM"
      },
      "source": [
        "sns.relplot(x=\"cuenta\",y=\"propina\",hue=\"hora\",row=\"sexo\",data=p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIbzWxzzVFJr"
      },
      "source": [
        "sns.relplot(x=\"cuenta\",y=\"propina\",hue=\"sexo\",col=\"hora\",row=\"dia\",data=p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQqgWxjwYtAo"
      },
      "source": [
        "# REGRESIONES LINEALES CON ANSCONMBE Y GRAFICA DE RESIDUALES\n",
        "a = sns.load_dataset(\"anscombe\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3OGMPZIYsbg"
      },
      "source": [
        "a.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QjqQegKZE1r"
      },
      "source": [
        "sns.regplot(x=\"x\",y=\"y\",data=a.query(\"dataset=='I' \"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pE-ssra6ZEiG"
      },
      "source": [
        "sns.residplot(x=\"x\",y=\"y\",data=a.query(\"dataset=='I'\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fehk3wCpZEPo"
      },
      "source": [
        "sns.regplot(x=\"x\",y=\"y\",data=a.query(\"dataset=='II'\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04RMguZzZD3c"
      },
      "source": [
        "sns.residplot(x=\"x\",y=\"y\",data=a.query(\"dataset=='II'\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLBc-kBBQKQ0"
      },
      "source": [
        "sns.regplot(x=\"x\",y=\"y\",data=a.query(\"dataset=='II'\"),order=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PToWT5qcRiG"
      },
      "source": [
        "sns.residplot(x=\"x\",y=\"y\",data=a.query(\"dataset=='II'\"),order=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOrU8spcTer3"
      },
      "source": [
        "# Tenga en cuenta que la estimación de la regresión logística es\n",
        "# considerablemente más computacionalmente intensiva (esto también es cierto\n",
        "# para la regresión robusta) que la regresión simple, y como el intervalo de\n",
        "# confianza alrededor de la línea de regresión se calcula utilizando un\n",
        "# procedimiento de arranque, es posible que desee desactivarlo para una\n",
        "# más rápido iteración (usando ci=None).\n",
        "\n",
        "# Un enfoque completamente diferente es ajustar una regresión no paramétrica\n",
        "# usando un lowess más suave\n",
        "# [lowess smoother](https://en.wikipedia.org/wiki/Local_regression).\n",
        "# Este enfoque tiene la menor cantidad de\n",
        "# supuestos, aunque es computacionalmente intensivo y, por lo tanto,\n",
        "# los intervalos de confianza actualmente no se calculan en absoluto:\n",
        "sns.lmplot(x=\"cuenta\", y=\"propina\", data=p,lowess=True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb7dRPXxEvpu"
      },
      "source": [
        "sns.residplot(x=\"cuenta\", y=\"propina\", data=p,\n",
        "           lowess=True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPFZOF5BVH6J"
      },
      "source": [
        "# The residplot() function can be a useful tool for checking whether the simple\n",
        "# regression model is appropriate for a dataset. It fits and removes a simple\n",
        "# linear regression and then plots the residual values for each observation.\n",
        "# Ideally, these values should be randomly scattered around y = 0:\n",
        "\n",
        "sns.residplot(x=\"x\", y=\"y\", data=a.query(\"dataset == 'I'\"),\n",
        "              scatter_kws={\"s\": 10});\n",
        "\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2y4zDVvuVH11"
      },
      "source": [
        "# If there is structure in the residuals, it suggests that simple linear\n",
        "# regression is not appropriate:\n",
        "\n",
        "sns.residplot(x=\"x\", y=\"y\", data=anscombe.query(\"dataset == 'II'\"),\n",
        "              scatter_kws={\"s\": 80});"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9piuAj8VlNR"
      },
      "source": [
        "#### Condicionamiento a otras variables\n",
        "\n",
        "---\n",
        "\n",
        "Las gráficas anteriores muestran muchas formas de explorar la relación entre un par de variables. Sin embargo, a menudo, una pregunta más interesante es \"¿cómo cambia la relación entre estas dos variables en función de una tercera variable?\" Aquí es donde aparece la diferencia entre **regplot()** y **lmplot()**.\n",
        "\n",
        "Mientras que **regplot()** siempre muestra una relación única, **lmplot()** combina **regplot()** con **FacetGrid** para proporcionar una interfaz fácil para mostrar una regresión lineal en gráficos \"facetados\" que le permiten explorar interacciones con hasta tres **variables categóricas** adicionales.\n",
        "\n",
        "---\n",
        "\n",
        "La mejor manera de separar una relación es trazar ambos niveles en los mismos ejes y usar el color para distinguirlos:\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJXsZ_tNVHxr"
      },
      "source": [
        "sns.lmplot(x=\"total_bill\", y=\"tip\", hue=\"smoker\", data=tips);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cftIjRVVHt0"
      },
      "source": [
        "# In addition to color, it’s possible to use different scatterplot markers\n",
        "# to make plots the reproduce to black and white better.\n",
        "# You also have full control over the colors used:\n",
        "\n",
        "sns.lmplot(x=\"total_bill\", y=\"tip\", hue=\"smoker\", data=tips,\n",
        "           markers=[\"o\", \"x\"], palette=\"Set1\");\n",
        "\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8riyAbqGVHoN"
      },
      "source": [
        "# To add another variable, you can draw multiple “facets” which each level\n",
        "# of the variable appearing in the rows or columns of the grid:\n",
        "\n",
        "sns.lmplot(x=\"total_bill\", y=\"tip\", hue=\"smoker\", col=\"time\", data=tips);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6V924CoHVHip"
      },
      "source": [
        "sns.lmplot(x=\"total_bill\", y=\"tip\", hue=\"smoker\",\n",
        "           col=\"time\", row=\"sex\", data=tips);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H5gg9XEJbtA"
      },
      "source": [
        "# PARA ESTUDIAR, wikipedia\n",
        "\n",
        "---\n",
        "\n",
        "[Regresión lineal](https://es.wikipedia.org/wiki/Regresi%C3%B3n_lineal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i90Utaxzy7rZ"
      },
      "source": [
        "# EJERCICIO: Verifique que la variable dependiente cloro esta relacionada linealmente con la variable independiente semanas.\n",
        "\n",
        "Los datos que se muestran en la siguiente celda, son los datos de Draper y Smith (1998), consisten de n = 44 muestras.\n",
        "\n",
        "Contiene datos de la cantidad de cloro en muestras de un producto en función del número de semanas desde que se produjo.\n",
        "\n",
        "Puede descargar el libro [Aquí](https://www.academia.edu/32080496/Applied_Regression_Analysis_Norman_R._Draper_Harry_Smith_.pdf?auto=download)\n",
        "\n",
        "**Página 505: Chapter 24. an Introduction to Nonlinear Stimation**.\n",
        "\n",
        "**El dato y la expliación del experimento de la producción de cloro se encuentra en la página 519 en: Table 24.2. Percent of Available Chlorine in a Unit of Product.**\n",
        "\n",
        "El ejemplo se explica en la página 518 en la sección:\n",
        "**24.3 An Example** y la ecuación es la 24.3.2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQU25gcnsN8G"
      },
      "source": [
        "# [Statgraphics Online](http://statvision.com/statgraphics_online.htm)\n",
        "\n",
        "STATGRAPHICS Online es un paquete estadístico que se ejecuta dentro de un navegador web. Puede ingresar datos directamente en el editor de datos o leer datos de archivos de texto, archivos de Excel u otros formatos comunes. Los cálculos se realizan de forma remota en un servidor web y los resultados se devuelven a su navegador como HTML con imágenes gráficas incorporadas.\n",
        "\n",
        "El acceso a Statgraphics Online está disponible mediante la compra de una suscripción mensual. Los suscriptores pueden ingresar sus propios datos, cargar datos desde sus propias computadoras y crear y almacenar scripts. También puede hacer una demostración del programa como invitado, pero estará restringido a analizar solo los conjuntos de datos de muestra proporcionados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-1U2YEoxg0i"
      },
      "source": [
        "semanas=[8,8,10,10,10,10,12,12,12,12,14,14,14,16,16,16,18,18,\n",
        "        20,20,20,22,22,22,24,24,24,26,26,26,28,28,30,30,30,32,\n",
        "        32,34,36,36,38,38,40,42]\n",
        "\n",
        "cloro = [0.49,0.49,0.48,0.47,0.48,0.47,0.46,0.46,0.45,0.43,0.45,\n",
        "        0.43,0.43,0.44,0.43,0.43,0.46,0.45,0.42,0.42,0.43,0.41,\n",
        "        0.41,0.40,0.42,0.40,0.40,0.41,0.40,0.41,0.41,0.40,0.40,\n",
        "        0.40,0.38,0.41,0.40,0.40,0.41,0.38,0.40,0.40,0.39,0.39]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQxt_Lqe5anU"
      },
      "source": [
        "import seaborn as sns\n",
        "import scipy.stats\n",
        "from scipy.stats import shapiro\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ukkp8velZxi"
      },
      "source": [
        "# Crea un DataFrame de pandas con los vectores semanas y cloro\n",
        "d = {'semanas': semanas, 'cloro': cloro}\n",
        "df = pd.DataFrame(data=d)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TL2bvMMGdv6G"
      },
      "source": [
        "# Guardamos el data frame\n",
        "df.to_csv(\"cloro.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpeISnNLl_Cb"
      },
      "source": [
        "dir(scipy.stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRV-QUXJGa9M"
      },
      "source": [
        "# Miramos la normalidad de la variable dependiente:cloro\n",
        "from scipy.stats import shapiro\n",
        "\n",
        "# Realiza la prueba con la función shapiro\n",
        "s,p = shapiro(cloro)\n",
        "# Mira si p es menor que 0.05 se rechaza la normalidad de la variable\n",
        "if p < 0.05:\n",
        "    print(\"La variable cloro NO se distribuye normalmente: p-Valor = \",p)\n",
        "elif p >= 0.05:\n",
        "    print(\"La variable cloro SI se distribuye normalmente: p-Valor = \",p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smkXqsjal_x3"
      },
      "source": [
        "# Importo las funciones pearsonr(), spearmanr() y kendalltau() que realizan\n",
        "# una prueba para ver si hay o no correlación lineal entre dos variables cuantitativas\n",
        "from scipy.stats import pearsonr, spearmanr, kendalltau"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGCeDertojAc"
      },
      "source": [
        "# Devuelve dos valores el segundo valor es la probabilidad\n",
        "# Si este valor es menor que 0.05 entonces las dos variables\n",
        "# están correlacionadas\n",
        "pearsonr(df.semanas,df.cloro)     # Se usa si cloro es normal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bINZG2p06zI7"
      },
      "source": [
        "spearmanr(df.semanas,df.cloro)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ed8uzo46zj5"
      },
      "source": [
        "kendalltau(df.semanas,df.cloro)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2rB8QIso1bo"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ow-PxGFpQi8"
      },
      "source": [
        "df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5u34vw5epSlN"
      },
      "source": [
        "df.corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDVipbZApbJA"
      },
      "source": [
        "sns.relplot(x=\"semanas\",y=\"cloro\",data=df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j147EgStplru"
      },
      "source": [
        "sns.regplot(x=\"semanas\",y=\"cloro\",data=df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DHoabPbqFPE"
      },
      "source": [
        "sns.residplot(x=\"semanas\",y=\"cloro\",data=df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uUCDO6JEuPh"
      },
      "source": [
        "sns.regplot(x=\"semanas\",y=\"cloro\",data=df,order=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7xiHAd5FMh1"
      },
      "source": [
        "sns.residplot(x=\"semanas\",y=\"cloro\",data=df,order=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DVF4tbNFRnM"
      },
      "source": [
        "sns.regplot(x=\"semanas\",y=\"cloro\",data=df,robust=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lX_sb-MWFYC5"
      },
      "source": [
        "sns.residplot(x=\"semanas\",y=\"cloro\",data=df,robust=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXo7eAbKeLfP"
      },
      "source": [
        "[Ajuste no lineal](https://marceluda.github.io/python-para-fisicos/tuto/analisis/ajuste-no-lineal/)\n",
        "\n",
        "Se desea ajustar el siguiente modelo a los datos:\n",
        "\n",
        "$$\n",
        "chlorine =  a+(0.49-a)*\\exp^{(-b*(semanas-8))}\n",
        "$$\n",
        "\n",
        "Este modelo, sugerido por un experto en el área, contiene dos incógnitas: a, el valor asintótico basal que se alcanza con valores grandes de weeks, y b, la tasa exponencial de decaimiento.\n",
        "\n",
        "Puede descargar el libro [Aquí](https://www.academia.edu/32080496/Applied_Regression_Analysis_Norman_R._Draper_Harry_Smith_.pdf?auto=download)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeG6GTMmFcpk"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiNdGjjXBAhP"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.optimize import curve_fit\n",
        "\n",
        "# 1. Defino la función a ajustar\n",
        "def f(x, a, b):\n",
        "    return a + (0.49 -a)*np.exp(-b*(x-8))\n",
        "\n",
        "semanas=[8,8,10,10,10,10,12,12,12,12,14,14,14,16,16,16,18,18,\n",
        "        20,20,20,22,22,22,24,24,24,26,26,26,28,28,30,30,30,32,\n",
        "        32,34,36,36,38,38,40,42]\n",
        "\n",
        "cloro = [0.49,0.49,0.48,0.47,0.48,0.47,0.46,0.46,0.45,0.43,0.45,\n",
        "        0.43,0.43,0.44,0.43,0.43,0.46,0.45,0.42,0.42,0.43,0.41,\n",
        "        0.41,0.40,0.42,0.40,0.40,0.41,0.40,0.41,0.41,0.40,0.40,\n",
        "        0.40,0.38,0.41,0.40,0.40,0.41,0.38,0.40,0.40,0.39,0.39]\n",
        "\n",
        "# 2. Obtengo las variables involucradas\n",
        "x = semanas\n",
        "y = cloro\n",
        "\n",
        "# Invoco la función curve_fit()\n",
        "res,cov = curve_fit(f,x,y)\n",
        "\n",
        "print(res)\n",
        "print(cov)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5GQkPqQBATT"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "xx = np.linspace(8,40,50)\n",
        "fig,axes=plt.subplots()\n",
        "axes.scatter(x,y)\n",
        "axes.plot(xx,f(xx,0.39014001,0.1016327))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1_S5UwoA__0"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmq8v-LFVrla"
      },
      "source": [
        "# **TALLER REGRESION LINEAL Y NO LINEAL CON R**\n",
        "\n",
        "SUGERENCIAS:\n",
        "1. Puede utilizar R magit aquí en colaboratory, ó\n",
        "2. Desarrolle el notebook de R en kaggle para mayor comodidad\n",
        "\n",
        "**Objetivos**. Ajustar los datos de secado en función del tiempo a modelos lineales y no lineales\n",
        "\n",
        "**PROCEDIMIENTO**\n",
        "1. Cargue los datos del dataset “Humedades.xls”\n",
        "2. Seleccione los datos en Excel de tiempo y humedad para la temperatura de 40 grados y cree un dataset h40\n",
        "3. Haga lo mismo para las demás temperaturas y cree los datasets h50, h60, h70 y h80\n",
        "4. Intente ajustar los lineas suponiendo una relación lineal (Recuerde que el secado sigue una caída exponencial de acuerdo a la ley de enfriamiento de Newton)\n",
        "5. Ajuste a cada uno de los siguientes modelos:\n",
        "\n",
        "**MODELO DE HENDERSON & PABIS**\n",
        "$$\n",
        "r1 <- nls(humedad ~  a * exp(-k * tiempo), trace=TRUE)\n",
        "$$\n",
        "\n",
        "**MODELO DE LEWIS (a=1)**\n",
        "$$\n",
        "r2 <- nls(humedad ~  exp(-k * tiempo), trace=TRUE)\n",
        "$$\n",
        "\n",
        "**MODELO DE PAGE**\n",
        "$$\n",
        "r3 <- nls(humedad ~  exp(-k * tiempo^n), trace=TRUE)\n",
        "$$\n",
        "\n",
        "**MODELO DE LOS DOS TÉRMINOS**\n",
        "$$\n",
        "r4 <- nls(humedad ~  A1*exp(-B * tiempo) + A2*exp(B*tiempo), trace=TRUE)\n",
        "$$\n",
        "\n",
        "**MODELO DE PAGE MODIFICADO**\n",
        "$$\n",
        "r5 <- nls(humedad ~  exp((k * tiempo)^n), trace=TRUE)\n",
        "$$\n",
        "\n",
        "**MODELO DE THOMPSON**\n",
        "$$\n",
        "r6 <- nls(humedad ~ a*log(tiempo)+b*(log(tiempo))^2, trace=TRUE)\n",
        "$$\n",
        "\n",
        "**MODELO DE WANG Y SINGH**\n",
        "$$\n",
        "r7 <- nls(humedad ~ 1+a*tiempo+b*tiempo^2, trace=TRUE)\n",
        "$$\n",
        "\n",
        "**hacemos un anova para ver cual modelo se ajusta mejor, significancia de los modelos**\n",
        "\n",
        "**LA BONDAD DEL AJUSTE SERÁ PARA EL MAYOR R^2 Y LOS MENORES ssr y sse**\n",
        "\n",
        "a=anova(r1, r2, r3, r4,r7)\n",
        "\n",
        "**SIGNIFICANCIA DEL MODELO**\n",
        "**LA BONDAD DEL AJUSTE SERÁ PARA EL MAYOR R² Y LOS MENORES ssr y sse**\n",
        "\n",
        "**EL CALCULO DE R²: VER MONTGOMERY PAGINA 464**\n",
        "$$\n",
        "R^2=SSr/SSt=1-SSe/SSt\n",
        "$$\n",
        "\n",
        "$$\n",
        "r_2r1=1-sum(resid(r1)^2)/sum((humedad-mean(humedad))^2)\n",
        "$$\n",
        "\n",
        "**R^2: VER MONTGOMERY  PAGINA 464**\n",
        "\n",
        "$$\n",
        "sser1=sum(resid(r1)^2)\n",
        "$$\n",
        "\n",
        "$$\n",
        "sstr1=sum((humedad-mean(humedad))^2)\n",
        "$$\n",
        "\n",
        "$$\n",
        "ssrr1=sstr1-sser1\n",
        "$$\n",
        "**VER MONTGOMERY PAGINA 451**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSwSRaW4Fxf3"
      },
      "source": [
        "# EJERCICIO: En el conjunto de datos agregados, Verifique que la variable dependiente rcompc (Resistencia a la compresión del concreto en kg/cm2) está relacionada linealmente con la variable independiente rcompa (Resistencia a la compresión del agregado en kg/cm2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4riJ8wOtH7bw"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7gKAvFIH-7n"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAyGW2B7IqvT"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNT3Z14tJlvm"
      },
      "source": [
        "# Cargo el archivo de excel\n",
        "archivo = pd.ExcelFile(\"/content/drive/My Drive/2020_SEM_1/DATOS_2020_SEM_1.xlsx\")\n",
        "# Muestro los nombres de las hojas, para ver cómo están escritas\n",
        "archivo.sheet_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5ppGyYXKYAJ"
      },
      "source": [
        "# Conocido el nombre de la hoja que me interesa la cargo con la función parse()\n",
        "agregados = archivo.parse(\"Agregados\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfHoCYfiIuEt"
      },
      "source": [
        "# Si conozco el nombre de la hoja puedo cargarla pasando su nombre\n",
        "# en el argumento sheet_name=, utilizando la función read_excel de pandas\n",
        "agregados = pd.read_excel(\"/content/drive/My Drive/2020_SEM_1/DATOS_2020_SEM_1.xlsx\",sheet_name=\"Agregados\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azuUjZugI-SW"
      },
      "source": [
        "# Miro las primeras lineas\n",
        "agregados.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRKAORU2K2AN"
      },
      "source": [
        "# Importo el módulo seaborn\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZDbRwbpLEIr"
      },
      "source": [
        "# Trazo el gráfico de dispersión\n",
        "sns.relplot(x=\"rcompa\",y=\"rcompc\",data=agregados)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD0XjHghLWyx"
      },
      "source": [
        "# Importo las funciones para probar la normalidad y la correlación\n",
        "from scipy.stats import shapiro, pearsonr, spearmanr,kendalltau"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGAeraDCLxcT"
      },
      "source": [
        "# Pruebo la hipótesis nula de rcompc es normal\n",
        "shapiro(agregados.rcompc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIi0YzNLL3Pv"
      },
      "source": [
        "# Pruebo la hipótesis de rcompa y recompc están correlacionadas linealmente\n",
        "# Uso spearmanr o kendalltau porque no es normal rcompc\n",
        "spearmanr(agregados.rcompa,agregados.rcompc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnUBPCXaMlgA"
      },
      "source": [
        "sns.regplot(x=\"rcompa\",y=\"rcompc\",data=agregados)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NEt4VFyNCTG"
      },
      "source": [
        "sns.residplot(x=\"rcompa\",y=\"rcompc\",data=agregados)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCGUkt9MNLK3"
      },
      "source": [
        "sns.regplot(x=\"rcompa\",y=\"rcompc\",data=agregados,robust=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZsVdsFPNriV"
      },
      "source": [
        "sns.residplot(x=\"rcompa\",y=\"rcompc\",data=agregados,robust=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}